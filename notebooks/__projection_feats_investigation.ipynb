{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_GPU = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "from matplotlib.colors import ListedColormap\n",
    "from omegaconf import OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DIR = os.path.dirname(os.getcwd())\n",
    "ROOT = os.path.join(DIR, \"..\")\n",
    "sys.path.insert(0, ROOT)\n",
    "sys.path.insert(0, DIR)\n",
    "from torch_points3d.datasets.segmentation.multimodal.s3dis import S3DISFusedDataset\n",
    "from torch_points3d.datasets.segmentation.multimodal import IGNORE_LABEL\n",
    "\n",
    "_ = torch.cuda.is_available()\n",
    "_ = torch.cuda.memory_allocated()\n",
    "torch.cuda.set_device(I_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from torch_points3d.utils.config import hydra_read\n",
    "\n",
    "# Set root to the DATA drive, where the data was downloaded\n",
    "# DATA_ROOT = '/mnt/fa444ffd-fdb4-4701-88e7-f00297a8e29b/projects/datasets/s3dis'  # ???\n",
    "# DATA_ROOT = '/media/drobert-admin/DATA/datasets/s3dis'  # IGN DATA\n",
    "DATA_ROOT = '/media/drobert-admin/DATA2/datasets/s3dis'  # IGN DATA2\n",
    "# DATA_ROOT = '/var/data/drobert/datasets/s3dis'  # AI4GEO\n",
    "# DATA_ROOT = '/home/qt/robertda/scratch/datasets/s3dis'  # CNES\n",
    "# DATA_ROOT = '/raid/datasets/pointcloud/data/s3dis'  # ENGIE\n",
    "\n",
    "overrides = [\n",
    "    'task=segmentation',\n",
    "    'data=segmentation/multimodal/s3disfused/no3d_5cm_768x384-exact',\n",
    "    'models=segmentation/multimodal/no3d',\n",
    "    'model_name=RGB_ResNet18PPM_mean-feat',\n",
    "    'data.fold=5',\n",
    "    'data.sample_per_epoch=2000',\n",
    "    f\"data.dataroot={os.path.join(DATA_ROOT, '5cm_exact_768x384')}\",\n",
    "]\n",
    "\n",
    "cfg = hydra_read(overrides)\n",
    "\n",
    "# print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load S3DIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "    'ceiling',\n",
    "    'floor',\n",
    "    'wall',\n",
    "    'beam',\n",
    "    'column',\n",
    "    'window',\n",
    "    'door',\n",
    "    'chair',\n",
    "    'table',\n",
    "    'bookcase',\n",
    "    'sofa',\n",
    "    'board',\n",
    "    'clutter',\n",
    "]\n",
    "OBJECT_COLOR = [\n",
    "    [180, 180, 80],  #'ceiling' .-> .yellow\n",
    "    [95, 156, 196],  #'floor' .-> . blue\n",
    "    [179, 116, 81],  #'wall'  ->  brown\n",
    "    [241, 149, 131],  #'beam'  ->  salmon\n",
    "    [81, 163, 148],  #'column'  ->  bluegreen\n",
    "    [77, 174, 84],  #'window'  ->  bright green\n",
    "    [108, 135, 75],  #'door'   ->  dark green\n",
    "    [41, 49, 101],  #'chair'  ->  darkblue\n",
    "    [79, 79, 76],  #'table'  ->  dark grey\n",
    "    [223, 52, 52],  #'bookcase'  ->  red\n",
    "    [89, 47, 95],  #'sofa'  ->  purple\n",
    "    [81, 109, 114],  #'board'   ->  grey\n",
    "    [125, 125, 125],  #'clutter'  ->  light grey\n",
    "]\n",
    "\n",
    "num_classes = len(CLASSES)\n",
    "\n",
    "PROJ_FEATS = [\n",
    "    'normalized depth',\n",
    "    'linearity',\n",
    "    'planarity',\n",
    "    'scattering',\n",
    "    'orientation to the surface',\n",
    "    'normalized pixel height',\n",
    "    'density',\n",
    "    'occlusion'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "    \n",
    "dataset = S3DISFusedDataset(cfg.data)\n",
    "# print(dataset)\n",
    "\n",
    "print(f\"Time = {time() - start:0.1f} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove pixel memory credit transform\n",
    "from torch_points3d.core.data_transform.multimodal.image import PickImagesFromMemoryCredit\n",
    "from torch_points3d.datasets.base_dataset_multimodal import BaseDatasetMM\n",
    "for x in [dataset.train_dataset, dataset.val_dataset, dataset.test_dataset[0]]:\n",
    "    x.transform_image = BaseDatasetMM.remove_multimodal_transform(x.transform_image, [PickImagesFromMemoryCredit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_points3d.models.model_factory import instantiate_model\n",
    "# \n",
    "# model = instantiate_model(cfg, dataset)\n",
    "# model = model.train().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.metrics.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "# checkpoint_dir = '/home/ign.fr/drobert-admin/Bureau/benchmark_checkpoints/benchmark-Res16UNet21-15_ResImage3_light_1_a4_concatenation-20210304_210217'\n",
    "# checkpoint_dir = '/home/ign.fr/drobert-admin/Bureau/benchmark_checkpoints/benchmark-Res16UNet21-15_ResImage3_light_1_mean_concatenation-20210301_230608'\n",
    "# checkpoint_dir = '/home/ign.fr/drobert-admin/Bureau/benchmark_checkpoints/XYZ+RGB_a4-dim_cat-1'\n",
    "# checkpoint_dir = '/home/ign.fr/drobert-admin/Bureau/benchmark_checkpoints/XYZ+RGB_mean_cat-1'\n",
    "# checkpoint_dir = '/home/ign.fr/drobert-admin/Bureau/benchmark_checkpoints/XYZ+RGB_attention_debug_fast/files'\n",
    "# checkpoint_dir = '/workspace/projects/torch-points3d/outputs/benchmark/benchmark-Res16UNet21-15_light-20210330_193749/wandb/run-20210330_193750-1ltttctz/files'\n",
    "checkpoint_dir = '/home/ign.fr/drobert-admin/Bureau/benchmark_checkpoints/RGB_light_drop-50_view-loss_fold5'\n",
    "\n",
    "# RGB light drop 50 trained on \"5cm exact 512x256\"\n",
    "# checkpoint_dir = '/home/drobert/projects/torch-points3d/outputs/benchmark/benchmark-Res16UNet21-15_light_drop-50_image-view-loss-20210419_193538'\n",
    "\n",
    "# RGB light drop 50 trained on \"5cm exact 768x384\"\n",
    "checkpoint_dir = '/home/drobert/projects/torch-points3d/outputs/benchmark/benchmark-Res16UNet21-15_light_drop-50_image-view-loss-20210428_165425'\n",
    "checkpoint_dir = '/home/drobert/projects/torch-points3d/outputs/benchmark/benchmark-RGB_D32-4_persistent-indrop-50_mean_view-20210518_141358'\n",
    "checkpoint_dir = '/home/drobert/projects/torch-points3d/outputs/benchmark/benchmark-RGB_D32-4_persistent-indrop-50_mean_view-20210517_230329'\n",
    "\n",
    "# Load model from checkpoint\n",
    "selection_stage = 'val' # train, val, test\n",
    "weight_name = 'loss_seg'  # miou, macc, acc, ..., latest\n",
    "checkpoint = ModelCheckpoint(checkpoint_dir, cfg.model_name, selection_stage, run_config=cfg, resume=False)\n",
    "model = checkpoint.create_model(dataset, weight_name=weight_name)\n",
    "model = model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate the save_last option to investigate AttentiveBimodalCSRPool module\n",
    "i_pool_branch = 0\n",
    "# i_pool_branch = 1\n",
    "model.backbone.down_modules[i_pool_branch].image.view_pool.save_last = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on TRAIN/VAL/TEST set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.multimodal.data import MMBatch\n",
    "batch = MMBatch.from_mm_data_list([dataset.test_dataset[0][2]])\n",
    "\n",
    "if not model.is_multimodal:\n",
    "    batch = batch.data\n",
    "\n",
    "model.set_input(batch, model.device)\n",
    "_ = model(batch)\n",
    "\n",
    "# gt = model.labels.cpu().numpy()\n",
    "# pred = model.output.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "batch.data.pred = model.output.argmax(dim=1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_points3d.visualization.multimodal_data import visualize_mm_data\n",
    "\n",
    "visualize_mm_data(batch, class_names=CLASSES, class_colors=OBJECT_COLOR, figsize=800, voxel=0.05, show_3d=True, show_2d=False, color_mode='y', alpha=3, pointsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.multimodal.data import MMBatch\n",
    "from torch_points3d.metrics.confusion_matrix import ConfusionMatrix\n",
    "from tqdm import tqdm\n",
    "from torch_points3d.modules.multimodal.pooling import BimodalCSRPool, AttentiveBimodalCSRPool, HeuristicBimodalCSRPool\n",
    "\n",
    "def inference(model, dataset, set_name='TEST', n_infer=1000):\n",
    "    if set_name.upper() == 'TRAIN':\n",
    "        dataset_ = dataset.train_dataset\n",
    "    elif set_name.upper() == 'VAL':\n",
    "        dataset_ = dataset.val_dataset\n",
    "    elif set_name.upper() == 'TEST':\n",
    "        dataset_ = dataset.test_dataset[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown set '{set_name.upper()}'\")\n",
    "\n",
    "    c = ConfusionMatrix(dataset.num_classes)\n",
    "\n",
    "    attention = model.backbone.down_modules[i_pool_branch].image.view_pool\n",
    "\n",
    "    idx = []\n",
    "    group_size = []\n",
    "    x_proj = []\n",
    "    x_mod = []\n",
    "    if isinstance(attention, AttentiveBimodalCSRPool):\n",
    "        K = []\n",
    "        Q = []\n",
    "        C = []\n",
    "        A = []\n",
    "        G = []\n",
    "    Y = []\n",
    "    Y_pred = []\n",
    "    count = 0\n",
    "\n",
    "    for i in tqdm(np.random.choice(len(dataset_), n_infer)):\n",
    "\n",
    "        # Skip these two sphere samples, they make the model crash\n",
    "        if i in [903, 1470, 1471] and set_name.upper() == 'TEST':\n",
    "            continue\n",
    "\n",
    "        batch = MMBatch.from_mm_data_list([dataset_[int(i)]])\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "    #     # TEMPORARY FIX TO DROP SOME LOCAL PROJECTION FEATURES\n",
    "    #     for s in range(batch.modalities['image'].num_settings):\n",
    "    #         batch.modalities['image'][s].mappings.features = batch.modalities['image'][s].mappings.features[:, :-2]\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        if not model.is_multimodal:\n",
    "            batch = batch.data\n",
    "\n",
    "        model.set_input(batch, model.device)\n",
    "        _ = model(batch)\n",
    "\n",
    "        gt = model.labels.cpu().numpy()\n",
    "        pred = model.output.argmax(dim=1).cpu().numpy()\n",
    "        c.count_predicted_batch(gt, pred)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # POOLING DATA\n",
    "        idx.append(attention._last_idx.detach().cpu() + count)\n",
    "        group_size.append(attention._last_view_num.detach().cpu())\n",
    "        x_proj.append(attention._last_x_proj.detach().cpu())\n",
    "        x_mod.append(attention._last_x_mod.detach().cpu())\n",
    "        if isinstance(attention, AttentiveBimodalCSRPool):\n",
    "            K.append(attention._last_K.detach().cpu())\n",
    "            Q.append(attention._last_Q.detach().cpu())\n",
    "            C.append(attention._last_C.detach().cpu())\n",
    "            A.append(attention._last_A.detach().cpu())\n",
    "            G.append(attention._last_G.detach().cpu())\n",
    "        Y.append(gt)\n",
    "        Y_pred.append(pred)\n",
    "#         count += attention._last_idx.detach().cpu().max() + 1\n",
    "        count += gt.shape[0]\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "    oa = np.round(c.get_overall_accuracy() * 100, decimals=2)\n",
    "    macc = np.round(c.get_mean_class_accuracy() * 100, decimals=2)\n",
    "    miou = np.round(c.get_average_intersection_union() * 100, decimals=2)\n",
    "    iou_dict = {dataset.INV_OBJECT_LABEL[k]: np.round(v * 100, decimals=1) \n",
    "                for k, v in enumerate(c.get_intersection_union_per_class()[0])}\n",
    "\n",
    "    print(f\"OA : {oa}\")\n",
    "    print(f\"macc : {macc}\")\n",
    "    print(f\"mIoU : {miou}\")\n",
    "    print(\"Per class IoU\")\n",
    "    for k, v in iou_dict.items():\n",
    "        print(f\"    {k:<9}: {v}\")\n",
    "    print()\n",
    "\n",
    "    idx = torch.cat(idx, dim=0)\n",
    "    group_size = torch.cat(group_size, dim=0)\n",
    "    x_proj = torch.cat(x_proj, dim=0)\n",
    "    x_mod = torch.cat(x_mod, dim=0)\n",
    "    if isinstance(attention, AttentiveBimodalCSRPool):\n",
    "        K = torch.cat(K, dim=0)\n",
    "        Q = torch.cat(Q, dim=0)\n",
    "        C = torch.cat(C, dim=0)\n",
    "        A = torch.cat(A, dim=0)\n",
    "        G = torch.cat(G, dim=0)\n",
    "    Y = torch.cat([torch.from_numpy(y) for y in Y], dim=0)\n",
    "    Y_pred = torch.cat([torch.from_numpy(y) for y in Y_pred], dim=0)\n",
    "\n",
    "    group_size_view = torch.repeat_interleave(group_size, group_size)\n",
    "    Y_view = torch.repeat_interleave(Y, group_size)\n",
    "    Y_pred_view = torch.repeat_interleave(Y_pred, group_size)\n",
    "    if isinstance(attention, AttentiveBimodalCSRPool):\n",
    "        G_view = torch.repeat_interleave(G, group_size)\n",
    "\n",
    "    # For pure-2d mean logit fusion models, x_mod carries the logits\n",
    "    Y_pred_view_indiv = torch.max(x_mod, dim=1).indices\n",
    "    \n",
    "    # Row-normalized confusion matrix - rows sum up to 1. Each row carries \n",
    "    # the distribution of predictions for an expected label. This \n",
    "    # illustrates how classes are misclassified.\n",
    "    confusion = np.zeros((dataset.num_classes, dataset.num_classes))\n",
    "    for i, j in zip(Y.numpy(), Y_pred.numpy()):\n",
    "        confusion[i, j] += 1\n",
    "    sns.heatmap(confusion / confusion.max(axis=1).reshape(-1, 1), xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "    plt.show()\n",
    "    print()\n",
    "    \n",
    "    # Wrap up everything in an output dictionary\n",
    "    out  = {\n",
    "        'oa': oa,\n",
    "        'macc': macc,\n",
    "        'miou': miou,\n",
    "        'iou_dict': iou_dict,\n",
    "        'idx': idx,\n",
    "        'group_size': group_size,\n",
    "        'x_proj': x_proj,\n",
    "        'x_mod': x_mod,\n",
    "        'Y': Y,\n",
    "        'Y_pred': Y_pred,\n",
    "        'group_size_view': group_size_view,\n",
    "        'Y_view': Y_view,\n",
    "        'Y_pred_view': Y_pred_view,\n",
    "        'Y_pred_view_indiv': Y_pred_view_indiv,\n",
    "    }\n",
    "\n",
    "    if isinstance(attention, AttentiveBimodalCSRPool):\n",
    "        out['K'] = K\n",
    "        out['Q'] = Q\n",
    "        out['C'] = C\n",
    "        out['A'] = A\n",
    "        out['G'] = G\n",
    "        out['G_view'] = G_view\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on TEST for average-pooled logit model\n",
    "out = inference(model, dataset, set_name='TEST', n_infer=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computed <span style=\"color:red\">**logits mean-pool score: 51.2 mIoU**</span> (RGB drop 50 on exact 768x384 Area 5 5cm, with $n_{infer}=1000$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on TEST for min-depth heuristic\n",
    "model.backbone.down_modules[i_pool_branch].image.view_pool = HeuristicBimodalCSRPool(mode='min', feat='normalized_depth', save_last=True)\n",
    "out = inference(model, dataset, set_name='TEST', n_infer=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on TEST for max-occlusion heuristic\n",
    "model.backbone.down_modules[i_pool_branch].image.view_pool = HeuristicBimodalCSRPool(mode='max', feat='occlusion', save_last=True)\n",
    "out = inference(model, dataset, set_name='TEST', n_infer=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on TEST for max-orientation heuristic\n",
    "model.backbone.down_modules[i_pool_branch].image.view_pool = HeuristicBimodalCSRPool(mode='max', feat='orientation_to_the_surface', save_last=True)\n",
    "out = inference(model, dataset, set_name='TEST', n_infer=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on TEST for max-planarity heuristic\n",
    "model.backbone.down_modules[i_pool_branch].image.view_pool = HeuristicBimodalCSRPool(mode='max', feat='planarity', save_last=True)\n",
    "out = inference(model, dataset, set_name='TEST', n_infer=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on TEST for depth < 0.6 heuristic\n",
    "from torch_points3d.core.data_transform.multimodal.image import PickMappingsFromProjectionFeatures\n",
    "t = PickMappingsFromProjectionFeatures(feat=PROJ_FEATS.index('normalized depth'), lower=None, upper=0.6)\n",
    "for x in [dataset.train_dataset, dataset.val_dataset, dataset.test_dataset[0]]:\n",
    "    x.transform_image.transforms.append(t)\n",
    "    \n",
    "out = inference(model, dataset, set_name='TEST', n_infer=500)\n",
    "\n",
    "for x in [dataset.train_dataset, dataset.val_dataset, dataset.test_dataset[0]]:\n",
    "    x.transform_image.transforms.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column, window, door, chair benefit from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on TEST for planarity > 0.5 heuristic\n",
    "from torch_points3d.core.data_transform.multimodal.image import PickMappingsFromProjectionFeatures\n",
    "t = PickMappingsFromProjectionFeatures(feat=PROJ_FEATS.index('planarity'), lower=0.5, upper=None)\n",
    "for x in [dataset.train_dataset, dataset.val_dataset, dataset.test_dataset[0]]:\n",
    "    x.transform_image.transforms.append(t)\n",
    "    \n",
    "out = inference(model, dataset, set_name='TEST', n_infer=500)\n",
    "\n",
    "for x in [dataset.train_dataset, dataset.val_dataset, dataset.test_dataset[0]]:\n",
    "    x.transform_image.transforms.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on TEST for orientation > 0.25 heuristic\n",
    "from torch_points3d.core.data_transform.multimodal.image import PickMappingsFromProjectionFeatures\n",
    "t = PickMappingsFromProjectionFeatures(feat=PROJ_FEATS.index('orientation to the surface'), lower=0.25, upper=None)\n",
    "for x in [dataset.train_dataset, dataset.val_dataset, dataset.test_dataset[0]]:\n",
    "    x.transform_image.transforms.append(t)\n",
    "    \n",
    "out = inference(model, dataset, set_name='TEST', n_infer=500)\n",
    "\n",
    "for x in [dataset.train_dataset, dataset.val_dataset, dataset.test_dataset[0]]:\n",
    "    x.transform_image.transforms.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As projection features could predict it, window and column benefit from this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess bounds on the multi-view pooling performance\n",
    "Here we take a closer look at the individual and multi-view predictions. We want to estimate an upper bound on the multi-view prediction performance, the probability of a single view of being right, the probability of views of a group to be in agreement, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_scatter\n",
    "\n",
    "idx_seen = torch.unique(idx)\n",
    "n_points = Y.shape[0]\n",
    "\n",
    "confusion = ConfusionMatrix(dataset.num_classes)\n",
    "idx_best_of_group = torch_scatter.scatter_max((Y_view == Y_pred_view_indiv).float(), idx, dim_size=n_points)[1][idx_seen]\n",
    "confusion.count_predicted_batch(Y[idx_seen].numpy(), Y_pred_view_indiv[idx_best_of_group].numpy())\n",
    "\n",
    "oa = np.round(confusion.get_overall_accuracy() * 100, decimals=2)\n",
    "macc = np.round(confusion.get_mean_class_accuracy() * 100, decimals=2)\n",
    "miou = np.round(confusion.get_average_intersection_union() * 100, decimals=2)\n",
    "iou_dict = {dataset.INV_OBJECT_LABEL[k]: np.round(v * 100, decimals=1) \n",
    "            for k, v in enumerate(confusion.get_intersection_union_per_class()[0])}\n",
    "\n",
    "print(f\"Unseen points: {100 * (1 - idx_seen.shape[0] / n_points):0.1f}%\")\n",
    "print(f\"OA : {oa}\")\n",
    "print(f\"macc : {macc}\")\n",
    "print(f\"mIoU : {miou}\")\n",
    "print(\"Per class IoU\")\n",
    "for k, v in iou_dict.items():\n",
    "    print(f\"    {k:<9}: {v}\")\n",
    "del confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computed <span style=\"color:red\">**multi-view upper bound: ~64.9 mIoU**</span> (RGB drop 50 on exact 768x384 Area 5 5cm, with 2 runs of $n_{infer}=1000$). \n",
    "\n",
    "When compared to XYZRGB*, this would hypothetically bring the following improvements: \n",
    "- ceiling  : $-2.00$\n",
    "- floor    : $-6.10$\n",
    "- wall     : $3.90$\n",
    "- beam     : $1.60$\n",
    "- column   : $10.90$\n",
    "- window   : $21.80$\n",
    "- door     : $14.40$\n",
    "- chair    : $-16.30$\n",
    "- table    : $-2.40$\n",
    "- bookcase : $7.70$\n",
    "- sofa     : $-31.90$\n",
    "- board    : $28.10$\n",
    "- clutter  : $4.80$\n",
    "\n",
    "That is to say, there is a lot to hope for classes such as window, door and board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_seen = torch.unique(idx)\n",
    "n_points = Y.shape[0]\n",
    "\n",
    "print(f\"N_points      : {n_points}\")\n",
    "print(f\"N_seen        : {idx_seen.shape[0]}\")\n",
    "print(f\"Unseen points : {100 * (1 - idx_seen.shape[0] / n_points):0.1f}%\")\n",
    "print(f\"N_views       : {Y_view.shape[0]}\")\n",
    "print()\n",
    "print(f\"CHECK - idx    increases           : {torch.all(idx[1:] >= idx[:-1])}\")\n",
    "print(f\"CHECK - idx    matches group_size  : {torch.all(torch.arange(n_points).repeat_interleave(group_size) == idx)}\")\n",
    "print(f\"CHECK - Y      matches Y_view      : {torch.all(torch.repeat_interleave(Y, group_size) == Y_view)}\")\n",
    "print(f\"CHECK - Y_pred matches Y_pred_view : {torch.all(torch.repeat_interleave(Y_pred, group_size) == Y_pred_view)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark on Y_pred_indiv: although not often, <span style=\"color:red\">there may be a discrepancy in the Y_view_pred (view-expanded mean-pooled prediction) and the Y_view_pred_indiv (view-wise individual predictions)</span>. It typically happens when the second highest logit is the same in the individual predictions, and they disagree on the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = ConfusionMatrix(dataset.num_classes)\n",
    "confusion.count_predicted_batch(Y_pred_view.numpy(), Y_pred_view_indiv.numpy())\n",
    "\n",
    "oa = np.round(confusion.get_overall_accuracy() * 100, decimals=2)\n",
    "macc = np.round(confusion.get_mean_class_accuracy() * 100, decimals=2)\n",
    "miou = np.round(confusion.get_average_intersection_union() * 100, decimals=2)\n",
    "iou_dict = {dataset.INV_OBJECT_LABEL[k]: np.round(v * 100, decimals=1) \n",
    "            for k, v in enumerate(confusion.get_intersection_union_per_class()[0])}\n",
    "\n",
    "print(f\"Unseen points: {100 * (1 - idx_seen.shape[0] / n_points):0.1f}%\")\n",
    "print(f\"OA : {oa}\")\n",
    "del confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OA here is interesting: it gives an idea of <span style=\"color:red\">how often the individual image predictions agree with the group's final prediction</span>. This, however is no garantee that the group's prediction is these cases is any bettter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = ConfusionMatrix(dataset.num_classes)\n",
    "confusion.count_predicted_batch(Y_view.numpy(), Y_pred_view_indiv.numpy())\n",
    "\n",
    "oa = np.round(confusion.get_overall_accuracy() * 100, decimals=2)\n",
    "macc = np.round(confusion.get_mean_class_accuracy() * 100, decimals=2)\n",
    "miou = np.round(confusion.get_average_intersection_union() * 100, decimals=2)\n",
    "iou_dict = {dataset.INV_OBJECT_LABEL[k]: np.round(v * 100, decimals=1) \n",
    "            for k, v in enumerate(confusion.get_intersection_union_per_class()[0])}\n",
    "\n",
    "print(f\"Unseen points: {100 * (1 - idx_seen.shape[0] / n_points):0.1f}%\")\n",
    "print(f\"OA : {oa}\")\n",
    "print(f\"macc : {macc}\")\n",
    "print(f\"mIoU : {miou}\")\n",
    "print(\"Per class IoU\")\n",
    "for k, v in iou_dict.items():\n",
    "    print(f\"    {k:<9}: {v}\")\n",
    "del confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These measures give an idea of <span style=\"color:red\">how individual image predictions perform wrt the ground truth</span>. This illustrates how <span style=\"color:red\">30% of the time, what the images say is wrong</span>. And not images are equal. Would be good to link these erroneous predictions with any of the projection features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = ConfusionMatrix(dataset.num_classes)\n",
    "confusion.count_predicted_batch(Y.numpy(), Y_pred.numpy())\n",
    "\n",
    "oa = np.round(confusion.get_overall_accuracy() * 100, decimals=2)\n",
    "macc = np.round(confusion.get_mean_class_accuracy() * 100, decimals=2)\n",
    "miou = np.round(confusion.get_average_intersection_union() * 100, decimals=2)\n",
    "iou_dict = {dataset.INV_OBJECT_LABEL[k]: np.round(v * 100, decimals=1) \n",
    "            for k, v in enumerate(confusion.get_intersection_union_per_class()[0])}\n",
    "\n",
    "print(f\"Unseen points: {100 * (1 - idx_seen.shape[0] / n_points):0.1f}%\")\n",
    "print(f\"OA : {oa}\")\n",
    "print(f\"macc : {macc}\")\n",
    "print(f\"mIoU : {miou}\")\n",
    "print(\"Per class IoU\")\n",
    "for k, v in iou_dict.items():\n",
    "    print(f\"    {k:<9}: {v}\")\n",
    "del confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_scatter\n",
    "\n",
    "idx_seen = torch.unique(idx)\n",
    "n_points = Y.shape[0]\n",
    "\n",
    "confusion = ConfusionMatrix(dataset.num_classes)\n",
    "idx_best_of_group = torch_scatter.scatter_min((Y_view == Y_pred_view_indiv).float(), idx, dim_size=n_points)[1][idx_seen]\n",
    "confusion.count_predicted_batch(Y[idx_seen].numpy(), Y_pred_view_indiv[idx_best_of_group].numpy())\n",
    "\n",
    "oa = np.round(confusion.get_overall_accuracy() * 100, decimals=2)\n",
    "macc = np.round(confusion.get_mean_class_accuracy() * 100, decimals=2)\n",
    "miou = np.round(confusion.get_average_intersection_union() * 100, decimals=2)\n",
    "iou_dict = {dataset.INV_OBJECT_LABEL[k]: np.round(v * 100, decimals=1) \n",
    "            for k, v in enumerate(confusion.get_intersection_union_per_class()[0])}\n",
    "\n",
    "print(f\"Unseen points: {100 * (1 - idx_seen.shape[0] / n_points):0.1f}%\")\n",
    "print(f\"OA : {oa}\")\n",
    "print(f\"macc : {macc}\")\n",
    "print(f\"mIoU : {miou}\")\n",
    "print(\"Per class IoU\")\n",
    "for k, v in iou_dict.items():\n",
    "    print(f\"    {k:<9}: {v}\")\n",
    "del confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search worst-case examples for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.multimodal.data import MMBatch\n",
    "from torch_points3d.metrics.confusion_matrix import ConfusionMatrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "if SET.upper() == 'TRAIN':\n",
    "    dataset_ = dataset.train_dataset\n",
    "elif SET.upper() == 'VAL':\n",
    "    dataset_ = dataset.val_dataset\n",
    "elif SET.upper() == 'TEST':\n",
    "    dataset_ = dataset.test_dataset[0]\n",
    "else:\n",
    "    raise ValueError(f\"Unknown set '{SET.upper()}'\")\n",
    "    \n",
    "idx_worst_sample = np.zeros(dataset.num_classes, dtype='int')\n",
    "iou_worst_sample = np.ones(dataset.num_classes)\n",
    "idx_best_sample = np.zeros(dataset.num_classes, dtype='int')\n",
    "iou_best_sample = np.zeros(dataset.num_classes)\n",
    "\n",
    "n_points_min = 100\n",
    "\n",
    "for i in tqdm(range(len(dataset_))):\n",
    "    \n",
    "    if i in [903, 1470, 1471]:\n",
    "        continue\n",
    "    \n",
    "    batch = MMBatch.from_mm_data_list([dataset_[int(i)]])\n",
    "        \n",
    "    if not model.is_multimodal:\n",
    "        batch = batch.data\n",
    "    \n",
    "    model.set_input(batch, model.device)\n",
    "    _ = model(batch)\n",
    "    \n",
    "    gt = model.labels.cpu().numpy()\n",
    "    pred = model.output.argmax(dim=1).cpu().numpy()\n",
    "    \n",
    "    # Compute the local miou\n",
    "    c = ConfusionMatrix(dataset.num_classes)\n",
    "    c.count_predicted_batch(gt, pred)\n",
    "    \n",
    "    # Check whether the sampling contains the class in the GT\n",
    "    is_class_seen = c.confusion_matrix.sum(axis=1) > n_points_min\n",
    "    \n",
    "    # Compute the per-class IoU\n",
    "    iou = c.get_intersection_union_per_class()[0]\n",
    "    \n",
    "    # Update worse values\n",
    "    idx_worst_update = np.logical_and(iou < iou_worst_sample, is_class_seen)\n",
    "    iou_worst_sample[idx_worst_update] = iou[idx_worst_update]\n",
    "    idx_worst_sample[idx_worst_update] = i\n",
    "    \n",
    "    # Update best values\n",
    "    idx_best_update = np.logical_and(iou > iou_best_sample, is_class_seen)\n",
    "    iou_best_sample[idx_best_update] = iou[idx_best_update]\n",
    "    idx_best_sample[idx_best_update] = i\n",
    "    \n",
    "    print(f\"Best - Worst samples\")\n",
    "    for c, idx_b, idx_w in zip(CLASSES, idx_best_sample, idx_worst_sample):\n",
    "        print(f\"    {c:<9}: {idx_b} - {idx_w}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import Center, RandomRotate\n",
    "from torch_points3d.core.data_transform import RandomNoise, RandomScaleAnisotropic, RandomSymmetry, \\\n",
    "    DropFeature, XYZFeature, AddFeatsByKeys\n",
    "from torch_points3d.core.data_transform.multimodal.image import ToFloatImage, AddPixelHeightFeature, \\\n",
    "    PickImagesFromMappingArea, PickImagesFromMemoryCredit, CropImageGroups\n",
    "from torch_points3d.datasets.base_dataset import BaseDataset\n",
    "from torch_points3d.datasets.base_dataset_multimodal import BaseDatasetMM\n",
    "\n",
    "def sample_real_data(tg_dataset, idx=0):\n",
    "    \"\"\"\n",
    "    Temporarily remove the 3D and 2D transforms affecting the point \n",
    "    positions and images from the dataset to better visualize points \n",
    "    and images relative positions.\n",
    "    \"\"\"\n",
    "    transform = tg_dataset.transform\n",
    "    tg_dataset.transform = BaseDataset.remove_transform(transform, [Center, RandomNoise,\n",
    "        RandomRotate, RandomScaleAnisotropic, RandomSymmetry, DropFeature, AddFeatsByKeys])\n",
    "    \n",
    "    transform_image = tg_dataset.transform_image\n",
    "    tg_dataset.transform_image = BaseDatasetMM.remove_multimodal_transform(transform_image, [ToFloatImage, AddPixelHeightFeature])\n",
    "\n",
    "    out = tg_dataset[idx]\n",
    "    \n",
    "    tg_dataset.transform = transform\n",
    "    tg_dataset.transform_image = transform_image\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.visualization.multimodal_data import visualize_mm_data\n",
    "\n",
    "idx = 82\n",
    "\n",
    "# Compute the predicted labels\n",
    "batch = MMBatch.from_mm_data_list([dataset_[idx]])   \n",
    "if not model.is_multimodal:\n",
    "    batch = batch.data\n",
    "model.set_input(batch, model.device)\n",
    "_ = model(batch)\n",
    "y_pred = model.output.argmax(dim=1).cpu()\n",
    "\n",
    "# Visualize the ground truth\n",
    "mm_data = sample_real_data(dataset.test_dataset[0], idx=idx)\n",
    "visualize_mm_data(mm_data, class_names=CLASSES, class_colors=OBJECT_COLOR, figsize=800, voxel=0.05, show_3d=True, show_2d=True, color_mode='light', alpha=3, pointsize=5)\n",
    "\n",
    "# Visualize the predictions\n",
    "mm_data.data.y = y_pred\n",
    "visualize_mm_data(mm_data, class_names=CLASSES, class_colors=OBJECT_COLOR, figsize=800, voxel=0.05, show_3d=True, show_2d=True, color_mode='y', alpha=3, pointsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scattering unskewed\n",
    "# i_feat = 3\n",
    "# fig = plt.figure()\n",
    "# sns.distplot(np.power(np.random.choice(x_proj[:, i_feat][select].numpy(), N_PRINT), 1/3), label='True',)\n",
    "# sns.distplot(np.power(np.random.choice(x_proj[:, i_feat][~select].numpy(), N_PRINT), 1/3), label='False', )\n",
    "# plt.title(PROJ_FEATS[i_feat])\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Occlusion unskewed\n",
    "# i_feat = 7\n",
    "# fig = plt.figure()\n",
    "# sns.distplot(np.power(np.random.choice(x_proj[:, i_feat][select].numpy(), N_PRINT), 4), label='True',)\n",
    "# sns.distplot(np.power(np.random.choice(x_proj[:, i_feat][~select].numpy(), N_PRINT), 4), label='False', )\n",
    "# ax.set_title(PROJ_FEATS[i_feat])\n",
    "# ax.legend()\n",
    "# ax.set_ylabel('')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PRINT = 5000\n",
    "\n",
    "# Define the plots\n",
    "fig = plt.figure(figsize=(4*len(PROJ_FEATS), 4*(len(CLASSES)+1)))\n",
    "axes = fig.subplots(len(CLASSES)+1, len(PROJ_FEATS))\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_yticks([])\n",
    "\n",
    "for ax, col_name in zip(axes[0], PROJ_FEATS):\n",
    "    ax.set_title(col_name.upper(), fontsize=12)\n",
    "\n",
    "for ax, row_name in zip(axes[:,0], ['GLOBAL'] + [f\"{class_name.upper()} - {list(iou_dict.values())[i_class]} IoU\" for i_class, class_name in enumerate(CLASSES)]):\n",
    "    ax.set_ylabel(row_name.upper(), fontsize=12)\n",
    "\n",
    "# Plot the global distribution of projection features wrt prediction errors\n",
    "select = Y_pred_view_indiv == Y_view\n",
    "for i_feat, (ax, feat_name) in enumerate(zip(axes[0], PROJ_FEATS)):\n",
    "    sns.distplot(np.random.choice(x_proj[:, i_feat][select].numpy(), N_PRINT), label='True', ax=ax, color='black')\n",
    "    sns.distplot(np.random.choice(x_proj[:, i_feat][~select].numpy(), N_PRINT), label='False', ax=ax, color='red')\n",
    "    ax.legend()\n",
    "\n",
    "# Plot the per-class distribution of projection features wrt prediction errors\n",
    "for i_class, class_name in enumerate(CLASSES):\n",
    "    t = Y_view == i_class\n",
    "    p = Y_pred_view_indiv == i_class\n",
    "    tp = torch.logical_and(t, p)\n",
    "    fp = torch.logical_and(~t, p)\n",
    "    fn = torch.logical_and(t, ~p)\n",
    "\n",
    "    for i_feat, (ax, feat_name) in enumerate(zip(axes[i_class+1], PROJ_FEATS)):\n",
    "        for select, label, color in zip((tp, fp, fn), ('tp', 'fp', 'fn'), ('black', 'green', 'red')):\n",
    "            if select.sum() > 0:\n",
    "                sns.distplot(np.random.choice(x_proj[:, i_feat][select].numpy(), N_PRINT), label=label.upper(), ax=ax, color=color)\n",
    "        ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts\n",
    "The distribution of projection features wrt pure-2D model predictions suggest the former should be able to help disambiguate some cases. \n",
    "\n",
    "However, this intuition means we would build an **_attention/selection mechanism_** able to attend to the views, based on their **_projection features AND their class_**. Since we don't know the class, the mechanism should make use of the features. \n",
    "\n",
    "- But will the network be able to learn this ? Are we not thinking in reverse, expecting the network to already know what we would like it to learn ?\n",
    "- And what should the queries and keys be built upon ? Shouldn't the 2D features be taken into account in the queries ? For the pure-2D model, we can think of queries built from the 2D features, much like in a self-attention mechanism. Why shouldn't this apply to the multimodal architecture too then ?\n",
    "- Besides, since the geometric information - some of which is carried in the local geometric features of the projection - seems to help disambiguate some 2D features, w wouldn't the 2D encoder benefit from receiveing some of it ? Otherwise said: can the 2D features also be fed contracted 3D features / multimodal attention outputs ? \n",
    "- Since in Transformer networks, the self-attention Q and K are all computed from the same data, shouldn't we do the same ? Rather than 'hiding' the 2D feats and projection feats from the Queries and 'hiding' the 3D feats from the keys, shouldn't they all benefit from all ? Maybe right now either the Keys or the Queries just don't know enough to actually do anything useful ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "N_PRINT = 1000\n",
    "\n",
    "fig = plt.figure(figsize=((len(CLASSES)+1)*5, 5))\n",
    "axes = fig.subplots(1, len(CLASSES)+1)\n",
    "\n",
    "# Plot the global t-SNE distribution of projection features wrt prediction errors\n",
    "select = Y_pred_view_indiv == Y_view\n",
    "rng = np.random.default_rng()\n",
    "x_tsne = TSNE(n_components=2).fit_transform(np.vstack((\n",
    "    rng.choice(x_proj[select].numpy(), int(N_PRINT/2)),\n",
    "    rng.choice(x_proj[~select].numpy(), int(N_PRINT/2)))))\n",
    "\n",
    "axes[0].scatter(x_tsne[:int(N_PRINT/2), 0], x_tsne[:int(N_PRINT/2), 1], color=\"blue\", alpha=0.5, label='True')\n",
    "axes[0].scatter(x_tsne[int(N_PRINT/2):, 0], x_tsne[int(N_PRINT/2):, 1], color=\"red\", alpha=0.5, label='False')\n",
    "axes[0].legend()\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('GLOBAL'.upper(), fontsize=20)\n",
    "\n",
    "# Plot the per-class t-SNE distribution of projection features wrt prediction errors\n",
    "for i_class, class_name in enumerate(CLASSES):\n",
    "    \n",
    "    t = Y_view == i_class\n",
    "    p = Y_pred_view_indiv == i_class\n",
    "    tp = torch.logical_and(t, p)\n",
    "    fp = torch.logical_and(~t, p)\n",
    "    fn = torch.logical_and(t, ~p)\n",
    "    \n",
    "    x_tsne = TSNE(n_components=2).fit_transform(np.vstack(\n",
    "        [rng.choice(x_proj[x].numpy(), int(N_PRINT/3)) for x in (tp, fp, fn) if x.sum() > 0]\n",
    "        ))\n",
    "    \n",
    "    pointers = np.cumsum([0] + [int(N_PRINT/3) * (x.sum() > 0) for x in (tp, fp, fn)])\n",
    "        \n",
    "    for i_select, (label, color) in enumerate(zip(('tp', 'fp', 'fn'), ('black', 'green', 'red'))):\n",
    "        a = pointers[i_select]\n",
    "        b = pointers[i_select+1]\n",
    "        if a < b:\n",
    "            axes[i_class+1].scatter(x_tsne[a:b, 0], x_tsne[a:b, 1], color=color, alpha=0.3, label=label)\n",
    "    axes[i_class+1].legend()\n",
    "    axes[i_class+1].axis('off')\n",
    "    axes[i_class+1].set_title(f\"{class_name.upper()} - {list(iou_dict.values())[i_class]} IoU\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from the t-SNE distribution of projection features and error cases (**_on the test data_**), we can conclude the following:\n",
    "- GLOBAL: there is little to hope from a class-agnostic (or 2D-feature) attention mechanism on the projection features. That means <span style=\"color:red\">keys should be computed based on the projection features AND the 2D features</span>. What about <span style=\"color:red\">queries</span> ?\n",
    "- COLUMN, WINDOW, DOOR, CHAIR, TABLE, BOARD: may benefit from projection features\n",
    "- CEILING, FLOOR: maybe but unsure\n",
    "- WALL, BOOKCASE, CLUTTER: unlikely to benefit from the projection features\n",
    "- Would the classes with no correlation with projection features still benefit from a self-attention mechanism ?\n",
    "- Would the projeciton features disambiguation help special cases where the XYZ and XYZRGB* models fail, or would it just bring the 2D model closer to XYZRGB* ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = np.zeros((num_classes, num_classes))\n",
    "for i, j in zip(Y.numpy(), Y_pred.numpy()):\n",
    "    confusion[i, j] += 1\n",
    "\n",
    "# sns.heatmap(confusion / confusion.max(axis=0).reshape(1, -1), xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "# plt.show()\n",
    "\n",
    "# Row-normalized confusion matrix - rows sum up to 1. Each row carries \n",
    "# the distribution of predictions for an expected label. This \n",
    "# illustrates how classes are misclassified.\n",
    "sns.heatmap(confusion / confusion.max(axis=1).reshape(-1, 1), xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PRINT = 10000\n",
    "\n",
    "fig = plt.figure(figsize=(32, 4))\n",
    "(ax0, ax1, ax2, ax3, ax4, ax5) = fig.subplots(1, 6)\n",
    "\n",
    "sns.distplot(group_size.numpy()[:N_PRINT], kde_kws={'bw': 0.3}, bins=np.arange(21), label=\"Group Size\", color='purple', ax=ax0)\n",
    "ax0.axvline(x=group_size.numpy()[:N_PRINT].mean(), ls=':', c='red')\n",
    "\n",
    "sns.distplot(torch.clamp(C, -1, 1.5).numpy()[:N_PRINT], label=\"Compatibilities\", color='b', ax=ax1)\n",
    "\n",
    "sns.distplot(torch.clamp(A, -1, 1.5).numpy()[:N_PRINT], label=\"Attention\", ax=ax2)\n",
    "for k in range(1, 10):\n",
    "    ax2.axvline(x=1/k, ls=':', c='black')\n",
    "\n",
    "sns.distplot((A * torch.repeat_interleave(group_size, group_size)).numpy()[:N_PRINT], label=\"Attention * Group Size\", ax=ax3)\n",
    "    \n",
    "sns.distplot(torch.clamp(G, -1, 1.5).numpy()[:N_PRINT], label=\"Gating\", color='r', ax=ax4)\n",
    "\n",
    "sns.distplot(Y.numpy()[:N_PRINT], kde=False, label=\"Y\", ax=ax5)\n",
    "sns.distplot(Y_pred.numpy()[:N_PRINT], kde=False, label=\"Y_pred\", ax=ax5)\n",
    "\n",
    "for ax in (ax0, ax1, ax2, ax3, ax4, ax5):\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(32, 2))\n",
    "axes = fig.subplots(1, num_classes)\n",
    "for i_class, ax in enumerate(axes):\n",
    "    idx_class = torch.where(Y == i_class)\n",
    "    group_size_class = group_size[idx_class]\n",
    "    sns.distplot(\n",
    "        torch.clamp(group_size_class, 0, 10).numpy()[:N_PRINT],\n",
    "        kde_kws={'bw': 0.3}, \n",
    "        bins=np.arange(11),\n",
    "        label=f\"{CLASSES[i_class]}\", \n",
    "        color=tuple([x / 255. for x in OBJECT_COLOR[i_class]]),\n",
    "        ax=ax)\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('')\n",
    "    ax.axvline(x=group_size.numpy()[:N_PRINT].mean(), ls=':', c='black')\n",
    "    ax.axvline(x=group_size_class.numpy()[:N_PRINT].mean(), ls=':', c='red')\n",
    "plt.suptitle(\"Per-class group sizes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(32, 4))\n",
    "(ax0, ax1, ax2, ax3, ax4, ax5) = fig.subplots(1, 6)\n",
    "\n",
    "sns.distplot(K[(x_proj < 0.3).squeeze()].numpy()[:N_PRINT], label=\"K toxic\", ax=ax0)\n",
    "sns.distplot(K[(x_proj >= 0.3).squeeze()].numpy()[:N_PRINT], label=\"K safe\", ax=ax0)\n",
    "\n",
    "sns.distplot(Q[(x_proj < 0.3).squeeze()].numpy()[:N_PRINT], label=\"Q toxic\", ax=ax1)\n",
    "sns.distplot(Q[(x_proj >= 0.3).squeeze()].numpy()[:N_PRINT], label=\"Q safe\", ax=ax1)\n",
    "\n",
    "sns.distplot(C[(x_proj < 0.3).squeeze()].numpy()[:N_PRINT], label=\"Compatibility toxic\", ax=ax2)\n",
    "sns.distplot(C[(x_proj >= 0.3).squeeze()].numpy()[:N_PRINT], label=\"Compatibility safe\", ax=ax2)\n",
    "\n",
    "sns.distplot(A[(x_proj < 0.3).squeeze()].numpy()[:N_PRINT], label=\"Attention toxic\", ax=ax3)\n",
    "sns.distplot(A[(x_proj >= 0.3).squeeze()].numpy()[:N_PRINT], label=\"Attention safe\", ax=ax3)\n",
    "\n",
    "for k in range(2, 10):\n",
    "    idx_group = group_size_view == k\n",
    "    sns.distplot(A[idx_group][(x_proj[idx_group] < 0.3).squeeze()].numpy()[:N_PRINT], label=f\"Attention toxic k={k}\", ax=ax4)\n",
    "    \n",
    "for k in range(2, 10):\n",
    "    idx_group = group_size_view == k\n",
    "    sns.distplot(A[idx_group][(x_proj[idx_group] >= 0.3).squeeze()].numpy()[:N_PRINT], label=f\"Attention safe k={k}\", ax=ax5)\n",
    "\n",
    "for ax in (ax0, ax1, ax2, ax3, ax4, ax5):\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tp3d_dev] *",
   "language": "python",
   "name": "conda-env-tp3d_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
