{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_GPU = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "from matplotlib.colors import ListedColormap\n",
    "from omegaconf import OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DIR = os.path.dirname(os.getcwd())\n",
    "ROOT = os.path.join(DIR, \"..\")\n",
    "sys.path.insert(0, ROOT)\n",
    "sys.path.insert(0, DIR)\n",
    "from torch_points3d.datasets.segmentation.multimodal.s3dis import S3DISFusedDataset\n",
    "from torch_points3d.datasets.segmentation.multimodal import IGNORE_LABEL\n",
    "\n",
    "_ = torch.cuda.is_available()\n",
    "_ = torch.cuda.memory_allocated()\n",
    "torch.cuda.set_device(I_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from torch_points3d.utils.config import hydra_read\n",
    "\n",
    "# Set root to the DATA drive, where the data was downloaded\n",
    "# DATA_ROOT = '/mnt/fa444ffd-fdb4-4701-88e7-f00297a8e29b/projects/datasets/s3dis'  # ???\n",
    "# DATA_ROOT = '/media/drobert-admin/DATA/datasets/s3dis'  # IGN DATA\n",
    "DATA_ROOT = '/media/drobert-admin/DATA2/datasets/s3dis'  # IGN DATA2\n",
    "# DATA_ROOT = '/var/data/drobert/datasets/s3dis'  # AI4GEO\n",
    "# DATA_ROOT = '/home/qt/robertda/scratch/datasets/s3dis'  # CNES\n",
    "# DATA_ROOT = '/raid/datasets/pointcloud/data/s3dis'  # ENGIE\n",
    "\n",
    "overrides = [\n",
    "    'task=segmentation',\n",
    "    'data=segmentation/multimodal/s3disfused/3d_2d/sparse/no_pixel_height/5cm_1024x512-exact',\n",
    "    'models=segmentation/multimodal/no3d',\n",
    "    'model_name=XYZ-RGB-L4-late-QKV',\n",
    "    'data.fold=5',\n",
    "    'data.sample_per_epoch=2000',\n",
    "    f\"data.dataroot={os.path.join(DATA_ROOT, '5cm_exact_1024x512')}\",\n",
    "]\n",
    "\n",
    "cfg = hydra_read(overrides)\n",
    "\n",
    "# print(OmegaConf.to_yaml(cfg))# Load S3DIS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "    'ceiling',\n",
    "    'floor',\n",
    "    'wall',\n",
    "    'beam',\n",
    "    'column',\n",
    "    'window',\n",
    "    'door',\n",
    "    'chair',\n",
    "    'table',\n",
    "    'bookcase',\n",
    "    'sofa',\n",
    "    'board',\n",
    "    'clutter',\n",
    "]\n",
    "OBJECT_COLOR = [\n",
    "    [180, 180, 80],  #'ceiling' .-> .yellow\n",
    "    [95, 156, 196],  #'floor' .-> . blue\n",
    "    [179, 116, 81],  #'wall'  ->  brown\n",
    "    [241, 149, 131],  #'beam'  ->  salmon\n",
    "    [81, 163, 148],  #'column'  ->  bluegreen\n",
    "    [77, 174, 84],  #'window'  ->  bright green\n",
    "    [108, 135, 75],  #'door'   ->  dark green\n",
    "    [41, 49, 101],  #'chair'  ->  darkblue\n",
    "    [79, 79, 76],  #'table'  ->  dark grey\n",
    "    [223, 52, 52],  #'bookcase'  ->  red\n",
    "    [89, 47, 95],  #'sofa'  ->  purple\n",
    "    [81, 109, 114],  #'board'   ->  grey\n",
    "    [125, 125, 125],  #'clutter'  ->  light grey\n",
    "]\n",
    "\n",
    "num_classes = len(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "    \n",
    "dataset = S3DISFusedDataset(cfg.data)\n",
    "# print(dataset)\n",
    "\n",
    "# Remove pixel memory credit transform\n",
    "from torch_points3d.core.data_transform.multimodal.image import PickImagesFromMemoryCredit\n",
    "from torch_points3d.datasets.base_dataset_multimodal import BaseDatasetMM\n",
    "# dataset.test_dataset[0].transform_image = BaseDatasetMM.remove_multimodal_transform(dataset.test_dataset[0].transform_image, [PickImagesFromMemoryCredit])\n",
    "\n",
    "print(f\"Time = {time() - start:0.1f} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_points3d.models.model_factory import instantiate_model\n",
    "# \n",
    "# model = instantiate_model(cfg, dataset)\n",
    "# model = model.train().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.metrics.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "# checkpoint_dir = '/home/ign.fr/drobert-admin/Bureau/benchmark_checkpoints/benchmark-Res16UNet21-15_ResImage3_light_1_a4_concatenation-20210304_210217'\n",
    "# checkpoint_dir = '/home/ign.fr/drobert-admin/Bureau/benchmark_checkpoints/benchmark-Res16UNet21-15_ResImage3_light_1_mean_concatenation-20210301_230608'\n",
    "# checkpoint_dir = '/home/ign.fr/drobert-admin/Bureau/benchmark_checkpoints/XYZ+RGB_a4-dim_cat-1'\n",
    "# checkpoint_dir = '/home/ign.fr/drobert-admin/Bureau/benchmark_checkpoints/XYZ+RGB_mean_cat-1'\n",
    "# checkpoint_dir = '/home/ign.fr/drobert-admin/Bureau/benchmark_checkpoints/XYZ+RGB_attention_debug_fast/files'\n",
    "# checkpoint_dir = '/workspace/projects/torch-points3d/outputs/benchmark/benchmark-Res16UNet21-15_light-20210330_193749/wandb/run-20210330_193750-1ltttctz/files'\n",
    "# checkpoint_dir = '/home/ign.fr/drobert-admin/Bureau/benchmark_checkpoints/RGB_light_drop-50_view-loss_fold5'\n",
    "checkpoint_dir = '/home/ign.fr/drobert-admin/projects/torch-points3d/outputs/benchmark/cnes/XYZ+RGB/XYZ-RGB-L4-late-QKV'\n",
    "\n",
    "# RGB light drop 50 trained on \"5cm exact 512x256\"\n",
    "# checkpoint_dir = '/home/drobert/projects/torch-points3d/outputs/benchmark/benchmark-Res16UNet21-15_light_drop-50_image-view-loss-20210419_193538'\n",
    "\n",
    "# RGB light drop 50 trained on \"5cm exact 768x384\"\n",
    "# checkpoint_dir = '/home/drobert/projects/torch-points3d/outputs/benchmark/benchmark-Res16UNet21-15_light_drop-50_image-view-loss-20210428_165425'\n",
    "# checkpoint_dir = '/home/ign.fr/drobert-admin/projects/torch-points3d/outputs/benchmark/cnes/RGB/fold5/RGB_ResNet18PPM_gp-8-32-32+mod-128-1_gscale_LR-10-2-3-4_exact_1024x512'\n",
    "\n",
    "# Load model from checkpoint\n",
    "selection_stage = 'val' # train, val, test\n",
    "weight_name = 'loss_seg'  # miou, macc, acc, ..., latest\n",
    "checkpoint = ModelCheckpoint(checkpoint_dir, cfg.model_name, selection_stage, run_config=cfg, resume=False)\n",
    "model = checkpoint.create_model(dataset, weight_name=weight_name)\n",
    "model = model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate the save_last option to investigate QKVBimodalCSRPool module\n",
    "i_pool_branch = 0\n",
    "# i_pool_branch = 1\n",
    "# model.backbone.down_modules[i_pool_branch].image.view_pool.save_last = True\n",
    "model.backbone_no3d.down_modules[i_pool_branch].image.view_pool.save_last = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.multimodal.data import MMBatch\n",
    "from torch_points3d.metrics.confusion_matrix import ConfusionMatrix\n",
    "from tqdm import tqdm\n",
    "from torch_points3d.modules.multimodal.pooling import BimodalCSRPool, QKVBimodalCSRPool, HeuristicBimodalCSRPool, GroupBimodalCSRPool\n",
    "\n",
    "def inference(model, dataset, set_name='TEST', n_infer=1000):\n",
    "    if set_name.upper() == 'TRAIN':\n",
    "        dataset_ = dataset.train_dataset\n",
    "    elif set_name.upper() == 'VAL':\n",
    "        dataset_ = dataset.val_dataset\n",
    "    elif set_name.upper() == 'TEST':\n",
    "        dataset_ = dataset.test_dataset[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown set '{set_name.upper()}'\")\n",
    "\n",
    "    c = ConfusionMatrix(dataset.num_classes)\n",
    "    \n",
    "#     attention = model.backbone.down_modules[i_pool_branch].image.view_pool\n",
    "    attention = model.backbone_no3d.down_modules[i_pool_branch].image.view_pool\n",
    "    \n",
    "    idx = []\n",
    "    group_size = []\n",
    "    x_map = []\n",
    "    x_mod = []\n",
    "    if isinstance(attention, QKVBimodalCSRPool):\n",
    "        K = []\n",
    "        Q = []\n",
    "    C = []\n",
    "    A = []\n",
    "    G = []\n",
    "    Y = []\n",
    "    Y_pred = []\n",
    "    count = 0\n",
    "\n",
    "    for i in tqdm(np.random.choice(len(dataset_), n_infer)):\n",
    "\n",
    "        # Skip these two sphere samples, they make the model crash\n",
    "        if i in [1470, 1471] and set_name.upper() == 'TEST':\n",
    "            continue\n",
    "        \n",
    "        if i in [903] and set_name.upper() == 'TEST':\n",
    "            continue\n",
    "\n",
    "        batch = MMBatch.from_mm_data_list([dataset_[int(i)]])\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "    #     # TEMPORARY FIX TO DROP SOME LOCAL PROJECTION FEATURES\n",
    "    #     for s in range(batch.modalities['image'].num_settings):\n",
    "    #         batch.modalities['image'][s].mappings.features = batch.modalities['image'][s].mappings.features[:, :-2]\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        if not model.is_multimodal:\n",
    "            batch = batch.data\n",
    "\n",
    "        model.set_input(batch, model.device)\n",
    "        _ = model(batch)\n",
    "\n",
    "        gt = model.labels.cpu().numpy()\n",
    "        pred = model.output.argmax(dim=1).cpu().numpy()\n",
    "        c.count_predicted_batch(gt, pred)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # POOLING DATA\n",
    "        idx.append(attention._last_idx.detach().cpu() + count)\n",
    "        group_size.append(attention._last_view_num.detach().cpu())\n",
    "        x_map.append(attention._last_x_map.detach().cpu())\n",
    "        x_mod.append(attention._last_x_mod.detach().cpu())\n",
    "        if isinstance(attention, QKVBimodalCSRPool):\n",
    "            K.append(attention._last_K.detach().cpu())\n",
    "            Q.append(attention._last_Q.detach().cpu())\n",
    "        C.append(attention._last_C.detach().cpu())\n",
    "        A.append(attention._last_A.detach().cpu())\n",
    "        G.append(attention._last_G.detach().cpu())\n",
    "        Y.append(gt)\n",
    "        Y_pred.append(pred)\n",
    "        count += attention._last_idx.detach().cpu().max() + 1\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    oa = np.round(c.get_overall_accuracy() * 100, decimals=2)\n",
    "    macc = np.round(c.get_mean_class_accuracy() * 100, decimals=2)\n",
    "    miou = np.round(c.get_average_intersection_union() * 100, decimals=2)\n",
    "    iou_dict = {dataset.INV_OBJECT_LABEL[k]: np.round(v * 100, decimals=1) \n",
    "                for k, v in enumerate(c.get_intersection_union_per_class()[0])}\n",
    "\n",
    "    print(f\"OA : {oa}\")\n",
    "    print(f\"macc : {macc}\")\n",
    "    print(f\"mIoU : {miou}\")\n",
    "    print(\"Per class IoU\")\n",
    "    for k, v in iou_dict.items():\n",
    "        print(f\"    {k:<9}: {v}\")\n",
    "    print()\n",
    "    \n",
    "    idx = torch.cat(idx, dim=0)\n",
    "    group_size = torch.cat(group_size, dim=0)\n",
    "    x_map = torch.cat(x_map, dim=0)\n",
    "    x_mod = torch.cat(x_mod, dim=0)\n",
    "    if isinstance(attention, QKVBimodalCSRPool):\n",
    "        K = torch.cat(K, dim=0)\n",
    "        Q = torch.cat(Q, dim=0)\n",
    "    C = torch.cat(C, dim=0)\n",
    "    A = torch.cat(A, dim=0)\n",
    "    G = torch.cat(G, dim=0)\n",
    "    \n",
    "    Y = torch.cat([torch.from_numpy(y) for y in Y], dim=0)\n",
    "    Y_pred = torch.cat([torch.from_numpy(y) for y in Y_pred], dim=0)\n",
    "    \n",
    "    group_size_view = torch.repeat_interleave(group_size, group_size)\n",
    "    Y_view = torch.repeat_interleave(Y, group_size)\n",
    "    Y_pred_view = torch.repeat_interleave(Y_pred, group_size)\n",
    "    G_view = torch.repeat_interleave(G, group_size, dim=0)\n",
    "    \n",
    "    # For pure-2d mean logit fusion models, x_mod carries the logits\n",
    "    Y_pred_view_indiv = torch.max(x_mod, dim=1).indices\n",
    "    \n",
    "    # Row-normalized confusion matrix - rows sum up to 1. Each row carries \n",
    "    # the distribution of predictions for an expected label. This \n",
    "    # illustrates how classes are misclassified.\n",
    "    confusion = np.zeros((dataset.num_classes, dataset.num_classes))\n",
    "    for i, j in zip(Y.numpy(), Y_pred.numpy()):\n",
    "        confusion[i, j] += 1\n",
    "    sns.heatmap(confusion / confusion.max(axis=1).reshape(-1, 1), xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "    plt.show()\n",
    "    print()\n",
    "    \n",
    "    # Wrap up everything in an output dictionary\n",
    "    out  = {\n",
    "        'oa': oa,\n",
    "        'macc': macc,\n",
    "        'miou': miou,\n",
    "        'iou_dict': iou_dict,\n",
    "        'idx': idx,\n",
    "        'group_size': group_size,\n",
    "        'x_map': x_map,\n",
    "        'x_mod': x_mod,\n",
    "        'Y': Y,\n",
    "        'Y_pred': Y_pred,\n",
    "        'group_size_view': group_size_view,\n",
    "        'Y_view': Y_view,\n",
    "        'Y_pred_view': Y_pred_view,\n",
    "        'Y_pred_view_indiv': Y_pred_view_indiv,\n",
    "        'num_groups': attention.num_groups,\n",
    "    }\n",
    "    \n",
    "    if isinstance(attention, QKVBimodalCSRPool):\n",
    "        out['K'] = K\n",
    "        out['Q'] = Q\n",
    "    out['C'] = C\n",
    "    out['A'] = A\n",
    "    out['G'] = G\n",
    "    out['G_view'] = G_view\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on TEST for average-pooled logit model\n",
    "out = inference(model, dataset, set_name='TEST', n_infer=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 1D tensor 2D if need be\n",
    "make_1d_2d = lambda x: x.unsqueeze(1) if x is not None and x.dim() < 2 else x\n",
    "\n",
    "# Recover data\n",
    "oa = out.get('oa', None)\n",
    "macc = out.get('macc', None)\n",
    "miou = out.get('miou', None)\n",
    "iou_dict = out.get('iou_dict', None)\n",
    "idx = make_1d_2d(out.get('idx', None))\n",
    "group_size = out.get('group_size', None)\n",
    "x_proj = out.get('x_proj', None)\n",
    "x_mod = out.get('x_mod', None)\n",
    "Y = out.get('Y', None)\n",
    "Y_pred = out.get('Y_pred', None)\n",
    "group_size_view = out.get('group_size_view', None)\n",
    "Y_view = out.get('Y_view', None)\n",
    "Y_pred_view = out.get('Y_pred_view', None)\n",
    "Y_pred_view_indiv = out.get('Y_pred_view_indiv', None)\n",
    "K = make_1d_2d(out.get('K', None))\n",
    "Q = make_1d_2d(out.get('Q', None))\n",
    "C = make_1d_2d(out.get('C', None))\n",
    "A = make_1d_2d(out.get('A', None))\n",
    "G = make_1d_2d(out.get('G', None))\n",
    "G_view = make_1d_2d(out.get('G_view', None))\n",
    "num_groups = out.get('num_groups', None)\n",
    "\n",
    "# idx_compact values hold view indices so that all elements are seen \n",
    "img_idx = torch.unique(idx)\n",
    "if img_idx.shape[0] < idx.max() + 1:\n",
    "    idx_gen = torch.full((img_idx.max() + 1,), -1).long()\n",
    "    idx_gen = idx_gen.scatter_(0, img_idx, torch.arange(img_idx.shape[0]))\n",
    "    idx_compact = idx_gen[idx]\n",
    "else:\n",
    "    idx_compact = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_scatter\n",
    "\n",
    "# N_PRINT = 500\n",
    "N_PRINT = 50000\n",
    "\n",
    "fig = plt.figure(figsize=(32, 6))\n",
    "(ax0, ax1, ax2) = fig.subplots(1, 3)\n",
    "palette = sns.color_palette(\"muted\")\n",
    "hist_kws = {'element': 'step', 'stat': 'probability', 'kde': True}\n",
    "\n",
    "N_per_index = torch_scatter.scatter(torch.ones_like(A[:N_PRINT, 0]), idx_compact[:N_PRINT, 0], reduce='sum')\n",
    "N_per_src_element = N_per_index.gather(0, idx_compact[:N_PRINT, 0])\n",
    "H2 = torch_scatter.scatter((-A[:N_PRINT] * torch.log(A[:N_PRINT])), idx_compact[:N_PRINT], reduce='sum', dim=0) / N_per_index.log().unsqueeze(1)\n",
    "H2[torch.where(H2.isinf())] = 0  # for edge cases\n",
    "\n",
    "for i_group in range(num_groups):  \n",
    "    sns.histplot(H2[:, i_group].numpy(), color=palette[i_group], label=f'Normalized entropy group={i_group}', ax=ax0, **hist_kws)\n",
    "\n",
    "for i_group in range(num_groups):\n",
    "    sns.histplot((A[:N_PRINT, i_group] * N_per_src_element).numpy(), color=palette[i_group], label=f'A*N group={i_group}', ax=ax1, **hist_kws)\n",
    "\n",
    "for i_group in range(num_groups):\n",
    "    sns.histplot(A[:N_PRINT, i_group].numpy(), color=palette[i_group], label=f'A group={i_group}', ax=ax2, **hist_kws)\n",
    "\n",
    "for ax in (ax0, ax1, ax2):\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('')\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "Y_print = torch_scatter.scatter(Y_view[:N_PRINT], idx_compact[:N_PRINT, 0], reduce='mean').long()\n",
    "\n",
    "palette = sns.color_palette(\"muted\")\n",
    "hist_kws = {'element': 'step', 'stat': 'probability', 'kde': True}\n",
    "    \n",
    "for i_c, c in enumerate(Y_print.unique()):\n",
    "    \n",
    "    idx_class = torch.where(Y_print == c)[0]\n",
    "    \n",
    "    fig = plt.figure(figsize=(32, 6))\n",
    "    fig.suptitle(f\"Attention for '{CLASSES[c].upper()}' - N={idx_class.shape[0]}\")\n",
    "    (ax0, ax1, ax2) = fig.subplots(1, 3)\n",
    "    \n",
    "    for i_group in range(num_groups):  \n",
    "        sns.histplot(H2[:, i_group][idx_class].numpy(), color=palette[i_group], label=f'Normalied entropy group={i_group}', ax=ax0, **hist_kws)\n",
    "\n",
    "    for i_group in range(num_groups):\n",
    "        sns.histplot((A[:N_PRINT, i_group] * N_per_src_element)[idx_class].numpy(), color=palette[i_group], label=f'A*N group={i_group}', ax=ax1, **hist_kws)\n",
    "\n",
    "    for i_group in range(num_groups):\n",
    "        sns.histplot(A[:N_PRINT, i_group][idx_class].numpy(), color=palette[i_group], label=f'A group={i_group}', ax=ax2, **hist_kws)\n",
    "\n",
    "    for ax in (ax0, ax1, ax2):\n",
    "        ax.legend()\n",
    "        ax.set_ylabel('')\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PRINT = 50000\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4 * 7))\n",
    "(ax0, ax1, ax2, ax3, ax4, ax5, ax6) = fig.subplots(7, 1)\n",
    "\n",
    "palette = sns.color_palette(\"muted\")\n",
    "hist_kws = {'element': 'step', 'stat': 'probability', 'kde': True}\n",
    "\n",
    "# Multi-view bag size\n",
    "sns.histplot(group_size.numpy(), bins=np.arange(21), ax=ax0, element='step', stat='probability', kde=False)\n",
    "ax0.axvline(x=group_size.numpy().mean(), ls=':', c='black')\n",
    "ax0.title.set_text('Bag size')\n",
    "    \n",
    "for i_group in range(num_groups):\n",
    "    \n",
    "    # Compatibilities\n",
    "    sns.histplot(C[:N_PRINT, i_group].clamp(-1, 1.5).numpy(), color=palette[i_group], label=f'group={i_group}', ax=ax1, **hist_kws)\n",
    "    ax1.title.set_text('Compatibility')\n",
    "    \n",
    "    # Attention\n",
    "    sns.histplot(A[:N_PRINT, i_group].clamp(-1, 1.5).numpy(), color=palette[i_group], label=f'group={i_group}', ax=ax2, **hist_kws)\n",
    "    for k in range(1, 10):\n",
    "        ax2.axvline(x=1/k, ls=':', c='black')\n",
    "    ax2.title.set_text('Attention')\n",
    "    \n",
    "    # A * bag size\n",
    "    AN = A * torch.repeat_interleave(group_size, group_size, dim=0).unsqueeze(1)\n",
    "    sns.histplot(AN[:N_PRINT, i_group].numpy(), color=palette[i_group], label=f'group={i_group}', ax=ax3, **hist_kws)\n",
    "    ax3.title.set_text('Attention * Bag size')\n",
    "\n",
    "    # Gating\n",
    "    sns.histplot(G[:, i_group].clamp(-1, 1.5).numpy(), color=palette[i_group], label=f'group={i_group}', ax=ax4, **hist_kws)\n",
    "    ax4.title.set_text('Gating')\n",
    "    \n",
    "# Y\n",
    "sns.histplot(Y.numpy(), stat='probability', ax=ax5)\n",
    "ax5.title.set_text('Y')\n",
    "\n",
    "# Y_pred\n",
    "sns.histplot(Y_pred.numpy(), stat='probability', ax=ax6)\n",
    "ax6.title.set_text('Y_pred')\n",
    "\n",
    "for ax in (ax0, ax1, ax2, ax3, ax4, ax5, ax6):\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(32, 2))\n",
    "axes = fig.subplots(1, num_classes)\n",
    "for i_class, ax in enumerate(axes):\n",
    "    idx_class = torch.where(Y == i_class)\n",
    "    group_size_class = group_size[idx_class]\n",
    "    sns.distplot(\n",
    "        torch.clamp(group_size_class, 0, 10).numpy()[:N_PRINT],\n",
    "        kde_kws={'bw': 0.3}, \n",
    "        bins=np.arange(11),\n",
    "        label=f\"{CLASSES[i_class]}\", \n",
    "        color=tuple([x / 255. for x in OBJECT_COLOR[i_class]]),\n",
    "        ax=ax)\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('')\n",
    "    ax.axvline(x=group_size.numpy()[:N_PRINT].mean(), ls=':', c='black')\n",
    "    ax.axvline(x=group_size_class.numpy()[:N_PRINT].mean(), ls=':', c='red')\n",
    "plt.suptitle(\"Per-class group sizes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(32, 4))\n",
    "(ax0, ax1, ax2, ax3, ax4, ax5) = fig.subplots(1, 6)\n",
    "\n",
    "sns.distplot(K[(x_map < 0.3).squeeze()].numpy()[:N_PRINT], label=\"K toxic\", ax=ax0)\n",
    "sns.distplot(K[(x_map >= 0.3).squeeze()].numpy()[:N_PRINT], label=\"K safe\", ax=ax0)\n",
    "\n",
    "sns.distplot(Q[(x_map < 0.3).squeeze()].numpy()[:N_PRINT], label=\"Q toxic\", ax=ax1)\n",
    "sns.distplot(Q[(x_map >= 0.3).squeeze()].numpy()[:N_PRINT], label=\"Q safe\", ax=ax1)\n",
    "\n",
    "sns.distplot(C[(x_map < 0.3).squeeze()].numpy()[:N_PRINT], label=\"Compatibility toxic\", ax=ax2)\n",
    "sns.distplot(C[(x_map >= 0.3).squeeze()].numpy()[:N_PRINT], label=\"Compatibility safe\", ax=ax2)\n",
    "\n",
    "sns.distplot(A[(x_map < 0.3).squeeze()].numpy()[:N_PRINT], label=\"Attention toxic\", ax=ax3)\n",
    "sns.distplot(A[(x_map >= 0.3).squeeze()].numpy()[:N_PRINT], label=\"Attention safe\", ax=ax3)\n",
    "\n",
    "for k in range(2, 10):\n",
    "    idx_group = group_size_view == k\n",
    "    sns.distplot(A[idx_group][(x_map[idx_group] < 0.3).squeeze()].numpy()[:N_PRINT], label=f\"Attention toxic k={k}\", ax=ax4)\n",
    "    \n",
    "for k in range(2, 10):\n",
    "    idx_group = group_size_view == k\n",
    "    sns.distplot(A[idx_group][(x_map[idx_group] >= 0.3).squeeze()].numpy()[:N_PRINT], label=f\"Attention safe k={k}\", ax=ax5)\n",
    "\n",
    "for ax in (ax0, ax1, ax2, ax3, ax4, ax5):\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "batch = MMBatch.from_mm_data_list([dataset.test_dataset[0][int(i)]])\n",
    "    \n",
    "# ------------------------------------------------------------------\n",
    "# TEMPORARY FIX TO DROP SOME LOCAL PROJECTION FEATURES\n",
    "for s in range(batch.modalities['image'].num_settings):\n",
    "    batch.modalities['image'][s].mappings.features = batch.modalities['image'][s].mappings.features[:, :-2]\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "if not model.is_multimodal:\n",
    "    batch = batch.data\n",
    "\n",
    "model.set_input(batch, model.device)\n",
    "_ = model(batch)\n",
    "\n",
    "gt = model.labels.cpu().numpy()\n",
    "pred = model.output.argmax(dim=1).cpu().numpy()\n",
    "c.count_predicted_batch(gt, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention.Q.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention.Gating.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing softmax properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "x = np.random.normal(loc=0.0, scale=1.0, size=10000)\n",
    "\n",
    "sns.distplot(x)\n",
    "sns.distplot(np.exp(x) * x)\n",
    "plt.show()\n",
    "\n",
    "print(f\"mean x = {x.mean():0.2f}\")\n",
    "print(f\"mean x*exp(x) = {(np.exp(x) * x).mean():0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = 20\n",
    "size = 10**4\n",
    "\n",
    "x_sum = np.zeros(size)\n",
    "for i in range(n_items):\n",
    "    x = np.random.normal(loc=0.0, scale=1.0, size=size)\n",
    "    x = x * np.exp(x)\n",
    "    x_sum += x\n",
    "\n",
    "x_sum = x_sum / n_items\n",
    "\n",
    "sns.distplot(x_sum)\n",
    "plt.show()\n",
    "print(f\"mean sum(x*exp(x)) = {x_sum.mean():0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1, 30)\n",
    "plt.plot(np.exp(-x), label=\"exp(-x)\")\n",
    "plt.plot(1/x, label=\"x^-1\")\n",
    "plt.plot(np.power(x, -0.5), label=\"x^-0.5\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tp3d_dev] *",
   "language": "python",
   "name": "conda-env-tp3d_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
