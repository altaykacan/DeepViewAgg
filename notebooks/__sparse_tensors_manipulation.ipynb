{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_GPU = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import MinkowskiEngine as ME\n",
    "import torchsparse as TS\n",
    "torch.cuda.set_device(I_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = TS\n",
    "requires_grad = True\n",
    "\n",
    "\n",
    "n = 20\n",
    "dimension = 3\n",
    "in_feat_torch = 5\n",
    "in_feat_sparse = 2\n",
    "out_feat_sparse = 1\n",
    "n_batch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create coords and preprend batch id in first column\n",
    "coords = torch.randint(low=0, high=5, size=(n, dimension)).int()\n",
    "\n",
    "print(\"WARNING : MUST QUANTIZE BEFORE TORCHSPARSE SPARSETENSOR INSTANTIATION  (use ME quantization or GridSampling3d\")\n",
    "# You can use torchsparse.utils.sparse_quantize but it has a bug so need to edit torchsparse source locally...\n",
    "# this is just for experimenting here, be careful because unique() changes the index order \n",
    "coords = torch.unique(coords, dim=0)  # this is for torchsparse, which does not handle duplicates in input...\n",
    "\n",
    "batch = torch.randint(low=0, high=n_batch, size=(coords.shape[0], 1)).int()\n",
    "\n",
    "# Create features of the size of coords\n",
    "x = torch.rand(batch.shape[0], in_feat_torch, requires_grad=requires_grad).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass X in a torch operation\n",
    "torch_conv = torch.nn.Linear(in_features=in_feat_torch, out_features=in_feat_sparse, bias=True)\n",
    "XA = torch_conv(x)\n",
    "\n",
    "if lib == ME:\n",
    "\tbatch_coords = torch.cat((batch, coords), 1)  # in ME batch is the FIRST column  (torch_points3d/modules/SparseConv3d/nn/minkowski.py)\n",
    "else:\n",
    "\tbatch_coords = torch.cat((coords, batch), 1)  # in TS batch is the LAST column  (torch_points3d/modules/SparseConv3d/nn/torchsparse.py)\n",
    "    \n",
    "# Create the sparse tensors\n",
    "if lib == ME:\n",
    "\tA = lib.SparseTensor(XA, batch_coords, quantization_mode=lib.SparseTensorQuantizationMode.UNWEIGHTED_AVERAGE).to(device)\n",
    "else:\n",
    "\tA = lib.SparseTensor(XA, batch_coords).to(device)\n",
    "\tA.check()  # initialize the coord_maps\n",
    "# print(\"Sparse tensor A\")\n",
    "# print(f\"Num points : {A.C.shape[0]}\")\n",
    "# print(\"Sparse coordinates\\n\", A.C)\n",
    "# print(\"Features\\n\", A.F)\n",
    "# print()\n",
    "\n",
    "XB = torch.rand(batch_coords.shape[0], in_feat_sparse, requires_grad=requires_grad).float()\n",
    "if lib == ME:\n",
    "\tB = lib.SparseTensor(XB, batch_coords, quantization_mode=lib.SparseTensorQuantizationMode.UNWEIGHTED_AVERAGE).to(device)\n",
    "\t# B._manager = A.coordinate_manager  # ME v0.5\n",
    "\tB.coords_man = A.coords_man  # ME v0.4\n",
    "else:\n",
    "\tB = lib.SparseTensor(XB, batch_coords).to(device)\n",
    "\tB.check()  # initialize the coord_maps\n",
    "# print(\"Sparse tensor B\")\n",
    "# print(f\"Num points : {B.C.shape[0]}\")\n",
    "# print(\"Sparse coordinates\\n\", B.C)\n",
    "# print(\"Features\\n\", B.F)\n",
    "# print()\n",
    "\n",
    "\n",
    "# Sparse operations on the tensors\n",
    "C = A + B  # WARNING : SparseTensor s are assumed to have the same coords !\n",
    "# print(\"A + B            (assuming identical coordinates)\")\n",
    "# print(f\"Num points : {C.C.shape[0]}\")\n",
    "# print(\"Sparse coordinates\\n\", C.C)\n",
    "# print(\"Features\\n\", C.F)\n",
    "# print()\n",
    "\n",
    "\n",
    "if lib == ME:\n",
    "\tC = lib.cat(A, C)    # WARNING : SparseTensor s are assumed to have the same coords !\n",
    "else:\t\n",
    "\tC = lib.cat([A, C])  # WARNING : SparseTensor s are assumed to have the same coords !\n",
    "# print(\"cat(A, C)        (assuming identical coordinates)\")\n",
    "# print(f\"Num points : {C.C.shape[0]}\")\n",
    "# print(\"Sparse coordinates\\n\", C.C)\n",
    "# print(\"Features\\n\", C.F)\n",
    "# print()\n",
    "\n",
    "\n",
    "# Strided conv\n",
    "if lib == ME:\n",
    "\tsparse_conv_1 = ME.MinkowskiConvolution(in_channels=C.F.shape[1], out_channels=out_feat_sparse, kernel_size=3, stride=2, dimension=dimension).to(device)\n",
    "else:\n",
    "\timport torchsparse.nn as spnn\n",
    "\tsparse_conv_1 = spnn.Conv3d(in_channels=C.F.shape[1], out_channels=out_feat_sparse, kernel_size=3, stride=2, dilation=1).to(device)\n",
    "C = sparse_conv_1(C)\n",
    "# print(\"sparse_conv_1(C)\")\n",
    "# print(f\"Num points : {C.C.shape[0]}\")\n",
    "# print(\"Sparse coordinates\\n\", C.C)\n",
    "# print(\"Features\\n\", C.F)\n",
    "# print()\n",
    "\n",
    "\n",
    "# Non-strided conv\n",
    "if lib == ME:\n",
    "\tsparse_conv_2 = ME.MinkowskiConvolution(in_channels=C.F.shape[1], out_channels=out_feat_sparse, kernel_size=3, stride=1, dimension=dimension).to(device)\n",
    "else:\n",
    "\timport torchsparse.nn as spnn\n",
    "\tsparse_conv_2 = spnn.Conv3d(in_channels=C.F.shape[1], out_channels=out_feat_sparse, kernel_size=3, stride=1, dilation=1).to(device)\n",
    "D = sparse_conv_2(C)\n",
    "# print(\"sparse_conv_2(C)\")\n",
    "# print(f\"Num points : {D.C.shape[0]}\")\n",
    "# print(\"Sparse coordinates\\n\", D.C)\n",
    "# print(\"Features\\n\", C.F)\n",
    "# print()\n",
    "\n",
    "\n",
    "# Strided conv\n",
    "if lib == ME:\n",
    "\tsparse_conv_2 = ME.MinkowskiConvolution(in_channels=D.F.shape[1], out_channels=out_feat_sparse, kernel_size=3, stride=2, dimension=dimension).to(device)\n",
    "else:\n",
    "\timport torchsparse.nn as spnn\n",
    "\tsparse_conv_2 = spnn.Conv3d(in_channels=D.F.shape[1], out_channels=out_feat_sparse, kernel_size=3, stride=2, dilation=1).to(device)\n",
    "E = sparse_conv_2(D)\n",
    "# print(\"sparse_conv_2(C)\")\n",
    "# print(f\"Num points : {E.C.shape[0]}\")\n",
    "# print(\"Sparse coordinates\\n\", E.C)\n",
    "# print(\"Features\\n\", C.F)\n",
    "# print()\n",
    "\n",
    "\n",
    "Y = E.F.sum()\n",
    "\n",
    "\n",
    "# print(\"Before backward\")\n",
    "# print(\"[Is None] Input x grad : \", x.grad is None)\n",
    "# print(\"[Is None] Sparse conv kernel grad : \", list(sparse_conv_1.parameters())[0].grad is None)\n",
    "# print()\n",
    "\n",
    "\n",
    "# print(\"Y.backward    (only works on GPU if using torchsparse)\")\n",
    "# print()\n",
    "# Y.backward()  # will cause troubles for torchsparse on the CPU. GPU should be OK otherwise \n",
    "\n",
    "\n",
    "# print(\"After backward\")\n",
    "# print(\"[Is None] Input x grad : \", x.grad is None)\n",
    "# print(\"[Is None] Sparse conv kernel grad : \", list(sparse_conv_1.parameters())[0].grad is None)\n",
    "# print()\n",
    "# print(\"Input x grad\\n\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lib == ME:\n",
    "    # Using inverse mappings for input sparse nodes\n",
    "    # Allows restoring the mapping from quantized to input quantized in case there\n",
    "    # are duplicates in the input coords\n",
    "    print(\"Inverse_mapping maps to input quantized coords : \", torch.all(A.C[A.inverse_mapping] == batch_coords))  # ME v0.5\n",
    "\n",
    "    print(\"Unique index tells you which coords were taken in the input :\", A.unique_index)\n",
    "\n",
    "\n",
    "    # Input quantization ME v0.5\n",
    "    unique_map, inverse_map = ME.utils.quantization.quantize(batch_coords)\n",
    "    print(\"Quantization of input coords yields unique_map==A.unique_index and inverse_map\")\n",
    "    print(torch.all(A.unique_index == unique_map))\n",
    "\n",
    "\n",
    "    # coord_key : which sparse coordinate system resolution each tensor belongs to\n",
    "    print('Coords keys')\n",
    "    for st in [A, B, C, D, E]:\n",
    "        print(st.coords_key)\n",
    "    print()\n",
    "\n",
    "    print('Tensor stride')\n",
    "    for st in [A, B, C, D, E]:\n",
    "        print(st.tensor_stride)\n",
    "    print()\n",
    "\n",
    "    print('Tensor dimension')\n",
    "    for st in [A, B, C, D, E]:\n",
    "        print(st.D)\n",
    "    print()\n",
    "\n",
    "    print('Tensor feature shape')\n",
    "    for st in [A, B, C, D, E]:\n",
    "        print(st.size())\n",
    "    print()\n",
    "\n",
    "\n",
    "# Mappings\n",
    "if lib == ME:\n",
    "    print('Mapping from stride i to stride j')\n",
    "    cm = E.coords_man\n",
    "    src, target = cm.get_coords_map(1, 4)\n",
    "    print(f\"Source idx: {src}\")\n",
    "    print(f\"Target idx: {target}\")\n",
    "    print(f\"Source-sorted target idx: {target[src.argsort()]}\")\n",
    "\n",
    "if lib == TS :\n",
    "    from torchsparse.nn.functional import sphash, sphashquery\n",
    "    \n",
    "    strides = list(E.coord_maps.keys())\n",
    "    print(f\"Strides (resolutions) in the network : {strides}\")\n",
    "    \n",
    "    print('Mapping from stride i to stride j')\n",
    "    ratio = strides[-1] / strides[0]\n",
    "    in_coords = torch.floor(torch.floor(E.coord_maps[strides[0]].float() / ratio) * ratio).int()\n",
    "    out_coords = E.coord_maps[strides[-1]]\n",
    "    print(f\"in_coords: \\n{in_coords}\")\n",
    "    print(f\"D.coords\")\n",
    "    print(f\"out_coords: \\n{out_coords}\")\n",
    "    print(sphashquery(sphash(in_coords), sphash(out_coords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([2, 2, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 4])\n",
    "b = torch.Tensor([1, 1, 2, 2, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 3])\n",
    "\n",
    "for i in torch.unique(a):\n",
    "    idx = torch.where(a == i)[0]\n",
    "    print((b[idx] == (b[idx[0]] * torch.ones(idx.shape[0]))).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tp3d_dev] *",
   "language": "python",
   "name": "conda-env-tp3d_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
