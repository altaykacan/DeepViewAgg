{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScanNet\n",
    "\n",
    "This notebook lets you instantiate the **[ScanNet](http://www.scan-net.org/)** dataset from scratch and visualize **3D+2D room samples**.\n",
    "\n",
    "Note that you will need **at least 1.2T** available for the SanNet raw dataset and **at least 64G** for the processed files at **5cm voxel resolution** and **320x240 image resolution**. \n",
    "\n",
    "The ScanNet dataset is composed of **rooms** of video acquisitions of indoor scenes. Thes video streams were used to produce a point cloud and images.\n",
    "\n",
    "Each room is small enough to be loaded at once into a **64G RAM** memory. The `ScannetDatasetMM` class from `torch_points3d.datasets.segmentation.multimodal.scannet` deals with loading the room and part of the images of the associated video stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select you GPU\n",
    "I_GPU = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use autoreload\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from time import time\n",
    "from omegaconf import OmegaConf\n",
    "start = time()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.cuda.set_device(I_GPU)\n",
    "DIR = os.path.dirname(os.getcwd())\n",
    "ROOT = os.path.join(DIR, \"..\")\n",
    "sys.path.insert(0, ROOT)\n",
    "sys.path.insert(0, DIR)\n",
    "\n",
    "from torch_points3d.utils.config import hydra_read\n",
    "from torch_geometric.data import Data\n",
    "from torch_points3d.core.multimodal.data import MMData\n",
    "from torch_points3d.visualization.multimodal_data import visualize_mm_data\n",
    "from torch_points3d.core.multimodal.image import SameSettingImageData, ImageData\n",
    "from torch_points3d.datasets.segmentation.multimodal.scannet import ScannetDatasetMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `visualize_mm_data` does not throw any error but the visualization does not appear, you may need to change your plotly renderer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = 'jupyterlab'        # for local notebook\n",
    "# pio.renderers.default = 'iframe_connected'  # for remote notebook. Other working (but seemingly slower) options are: 'sphinx_gallery' and 'iframe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation\n",
    "\n",
    "The following will instantiate the dataset. If the data is not found at `DATA_ROOT`, the folder structure will be created there and the raw dataset will be downloaded there. \n",
    "\n",
    "**Memory-friendly tip** : if you have already downloaded the dataset once and simply want to instantiate a new dataset with different preprocessing (*e.g* change 3D or 2D resolution, mapping parameterization, etc), I recommend you manually replicate the folder hierarchy of your already-existing dataset and create a symlink to its `raw/` directory to avoid downloading and storing (very) large files twice.\n",
    "\n",
    "You will find the config file ruling the dataset creation at `conf/data/segmentation/multimodal/scannet-sparse.yaml`. You may edit this file or create new configs inheriting from this one using Hydra and create the associated dataset by modifying `dataset_config` accordingly in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your dataset root directory, where the data was/will be downloaded\n",
    "DATA_ROOT = '/path/to/your/dataset/root/directory'\n",
    "\n",
    "dataset_config = 'segmentation/multimodal/scannet-sparse'   \n",
    "models_config = 'segmentation/multimodal/sparseconv3d'    # this does not really matter here, but is expected by hydra for config parsing\n",
    "model_name = 'Res16UNet34-L4-early-ade20k-interpolate'    # this does not really matter here, but is expected by hydra for config parsing\n",
    "\n",
    "\n",
    "overrides = [\n",
    "    'task=segmentation',\n",
    "    f'data={dataset_config}',\n",
    "    f'models={models_config}',\n",
    "    f'model_name={model_name}',\n",
    "    f'data.dataroot={DATA_ROOT}',\n",
    "]\n",
    "\n",
    "cfg = hydra_read(overrides)\n",
    "# print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset will now be created based on the parsed configuration. I recommend having **at least 1.2T** available for the SanNet raw dataset and **at least 64G** for the processed files at **5cm voxel resolution** and **320x240 image resolution**. \n",
    "\n",
    "As long as you do not change core dataset parameters, preprocessing should only be performed once for your dataset. It may take some time, **mostly depending on the 3D and 2D resolutions** you choose to work with (the larger the slower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset instantiation\n",
    "start = time()\n",
    "dataset = ScannetDatasetMM(cfg.data)\n",
    "# print(dataset)\n",
    "print(f\"Time = {time() - start:0.1f} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the multimodal samples produced by the dataset, we need to remove some of the dataset transforms that affect points, images and mappings.\n",
    "\n",
    "At training and evaluation time, these transforms are used for data augmentation, dynamic size batching (see our [paper](https://arxiv.org/submit/4264152)), etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some 3D and 2D transforms to allow visualizations\n",
    "dataset.train_dataset.transform.transforms = dataset.train_dataset.transform.transforms[4:-1]\n",
    "dataset.train_dataset.transform_image.transforms = dataset.train_dataset.transform_image.transforms[:3]\n",
    "\n",
    "dataset.val_dataset.transform.transforms = dataset.val_dataset.transform.transforms[:2]\n",
    "dataset.val_dataset.transform_image.transforms = dataset.val_dataset.transform_image.transforms[:3]\n",
    "\n",
    "dataset.test_dataset[0].transform.transforms = dataset.test_dataset[0].transform.transforms[:2]\n",
    "dataset.test_dataset[0].transform_image.transforms = dataset.test_dataset[0].transform_image.transforms[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ScanNet proposes annotations for many classes but we are only interested in a subset of those. So we need no remap the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.segmentation.scannet import VALID_CLASS_IDS, SCANNET_COLOR_MAP, CLASS_LABELS, NUM_CLASSES, IGNORE_LABEL, CLASS_COLORS, CLASS_NAMES\n",
    "\n",
    "def remap_scannet_labels(semantic_label, valid_class_idx=VALID_CLASS_IDS, donotcare_class_ids=[]):\n",
    "    \"\"\"Remaps labels to [0 ; num_labels -1].\"\"\"\n",
    "    new_labels = semantic_label.clone()\n",
    "    mapping_dict = {idx: i for i, idx in enumerate(valid_class_idx)}\n",
    "    for idx in range(NUM_CLASSES):\n",
    "        if idx not in mapping_dict:\n",
    "            mapping_dict[idx] = IGNORE_LABEL\n",
    "    for idx in donotcare_class_ids:\n",
    "        mapping_dict[idx] = IGNORE_LABEL\n",
    "    for source, target in mapping_dict.items():\n",
    "        mask = semantic_label == source\n",
    "        new_labels[mask] = target\n",
    "\n",
    "    broken_labels = new_labels >= len(valid_class_idx)\n",
    "    new_labels[broken_labels] = IGNORE_LABEL\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a single multimodal sample\n",
    "\n",
    "We can now pick samples from the train, val and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_room = 0\n",
    "mm_data = dataset.train_dataset[i_room]    # pick a room in the Train set\n",
    "# mm_data = dataset.val_dataset[i_room]      # pick a room in the Val set\n",
    "# mm_data = dataset.test_dataset[0][i_room]  # pick a room in the Test set\n",
    "\n",
    "# Create MMData object, keep only a few images, remap labels and remove \n",
    "# IGNORE_LABEL points\n",
    "mm_data.data.y = remap_scannet_labels(mm_data.data.y)\n",
    "mm_data = mm_data[mm_data.data.y != IGNORE_LABEL]\n",
    "\n",
    "visualize_mm_data(mm_data, class_names=CLASS_NAMES, class_colors=CLASS_COLORS, front='y', figsize=1000, pointsize=3, voxel=0.05, show_2d=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tp3d_dev] *",
   "language": "python",
   "name": "conda-env-tp3d_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
