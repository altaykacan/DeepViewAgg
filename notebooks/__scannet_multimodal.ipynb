{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_GPU= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "from matplotlib.colors import ListedColormap\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "os.system('/usr/bin/Xvfb :99 -screen 0 1024x768x24 &')\n",
    "os.environ['DISPLAY'] = ':99'\n",
    "os.environ['PYVISTA_OFF_SCREEN'] = 'True'\n",
    "os.environ['PYVISTA_USE_PANEL'] = 'True'\n",
    "torch.cuda.set_device(I_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = os.path.dirname(os.getcwd())\n",
    "ROOT = os.path.join(DIR, \"..\")\n",
    "sys.path.insert(0, ROOT)\n",
    "sys.path.insert(0, DIR)\n",
    "# from torch_points3d.datasets.segmentation.scannet import ScannetDataset\n",
    "from torch_points3d.datasets.segmentation.multimodal.scannet import ScannetDatasetMM\n",
    "from torch_points3d.datasets.segmentation.multimodal import IGNORE_LABEL\n",
    "\n",
    "from torch_points3d.visualization.multimodal_data import visualize_mm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from torch_points3d.utils.config import hydra_read\n",
    "\n",
    "# Set root to the DATA drive, where the data was downloaded\n",
    "# DATA_ROOT = '/mnt/fa444ffd-fdb4-4701-88e7-f00297a8e29b/projects/datasets/scannet'  # ???\n",
    "# DATA_ROOT = '/media/drobert-admin/DATA/datasets/scannet'  # IGN DATA\n",
    "# DATA_ROOT = '/media/drobert-admin/DATA2/datasets/scannet'  # IGN DATA2\n",
    "# DATA_ROOT = '/var/data/drobert/datasets/scannet'  # AI4GEO\n",
    "# DATA_ROOT = '/home/qt/robertda/scratch/datasets/scannet'  # CNES\n",
    "DATA_ROOT = '/raid/dataset/pointcloud/data/scannet'  # ENGIE\n",
    "\n",
    "overrides = [\n",
    "    'task=segmentation',\n",
    "    'data=segmentation/multimodal/scannet-sparse',\n",
    "    'models=segmentation/multimodal/sparseconv3d',\n",
    "    'model_name=Res16UNet34-L4-L0',\n",
    "    f\"data.dataroot={os.path.join(DATA_ROOT, 'testing')}\",\n",
    "#     f\"data.dataroot={os.path.join(DATA_ROOT, 'mini')}\",\n",
    "    \n",
    "    'data.resolution_3d=0.05',\n",
    "    'data.resolution_2d=[320, 240]',\n",
    "    'data.exact_splatting_2d=True'\n",
    "]\n",
    "\n",
    "cfg = hydra_read(overrides)\n",
    "\n",
    "# print(OmegaConf.to_yaml(cfg))# Load S3DIS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Scannet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "\n",
    "dataset = ScannetDatasetMM(cfg.data)\n",
    "# print(dataset)\n",
    "\n",
    "print(f\"Time = {time() - start:0.1f} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.segmentation.scannet import VALID_CLASS_IDS, SCANNET_COLOR_MAP, CLASS_LABELS, NUM_CLASSES, IGNORE_LABEL, CLASS_COLORS, CLASS_NAMES\n",
    "from torch_points3d.visualization.multimodal_data import visualize_mm_data\n",
    "\n",
    "def remap_scannet_labels(semantic_label, valid_class_idx=VALID_CLASS_IDS, donotcare_class_ids=[]):\n",
    "    \"\"\"Remaps labels to [0 ; num_labels -1].\"\"\"\n",
    "    new_labels = semantic_label.clone()\n",
    "    mapping_dict = {idx: i for i, idx in enumerate(valid_class_idx)}\n",
    "    for idx in range(NUM_CLASSES):\n",
    "        if idx not in mapping_dict:\n",
    "            mapping_dict[idx] = IGNORE_LABEL\n",
    "    for idx in donotcare_class_ids:\n",
    "        mapping_dict[idx] = IGNORE_LABEL\n",
    "    for source, target in mapping_dict.items():\n",
    "        mask = semantic_label == source\n",
    "        new_labels[mask] = target\n",
    "\n",
    "    broken_labels = new_labels >= len(valid_class_idx)\n",
    "    new_labels[broken_labels] = IGNORE_LABEL\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some 3D and 2D transforms to allow visualizations\n",
    "dataset.train_dataset.transform.transforms = dataset.train_dataset.transform.transforms[4:-1]\n",
    "dataset.train_dataset.transform_image.transforms = dataset.train_dataset.transform_image.transforms[:3]\n",
    "\n",
    "dataset.val_dataset.transform.transforms = dataset.val_dataset.transform.transforms[:2]\n",
    "dataset.val_dataset.transform_image.transforms = dataset.val_dataset.transform_image.transforms[:3]\n",
    "\n",
    "dataset.test_dataset[0].transform.transforms = dataset.test_dataset[0].transform.transforms[:2]\n",
    "dataset.test_dataset[0].transform_image.transforms = dataset.test_dataset[0].transform_image.transforms[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data = dataset.train_dataset[0]\n",
    "# mm_data = dataset.val_dataset[0]\n",
    "# mm_data = dataset.test_dataset[0][0]\n",
    "\n",
    "# Create MMData object, keep only a few images, remap labels and remove \n",
    "# IGNORE_LABEL points\n",
    "# mm_data.data.y = remap_scannet_labels(mm_data.data.y)\n",
    "# mm_data = mm_data[mm_data.data.y != IGNORE_LABEL]\n",
    "\n",
    "# Prepare ScanNet colors and class names for visualization\n",
    "class_colors = CLASS_COLORS\n",
    "class_names = CLASS_NAMES\n",
    "\n",
    "visualize_mm_data(mm_data, class_names=class_names, class_colors=class_colors, front='y', figsize=1000, pointsize=3, voxel=0.05, show_2d=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "\n",
    "# image_idx = [20, 22, 24, 49]\n",
    "image_idx = [20, 24, 49]\n",
    "alpha = 0.5\n",
    "palette = [hex_to_tensor(c) for c in plotly.colors.qualitative.Plotly]\n",
    "\n",
    "# Create MMData object, keep only a few images, remap labels and remove \n",
    "# IGNORE_LABEL points\n",
    "mm_data_less = MMData(data_out.clone(), image=ImageData([image_data_out.clone()[image_idx]]))\n",
    "mm_data_less.data.y = remap_scannet_labels(mm_data_less.data.y)\n",
    "mm_data_less = mm_data_less[mm_data_less.data.y != IGNORE_LABEL]\n",
    "\n",
    "# Color the points wrt the images that see them, following the plotly\n",
    "# palette used to color image balls\n",
    "for i in range(len(image_idx)):\n",
    "    mapping = mm_data_less.modalities['image'][0].mappings.select_images(i)\n",
    "    idx_seen = mapping.pointers[1:] > mapping.pointers[:-1]\n",
    "    mm_data_less.data.rgb[idx_seen] = mm_data_less.data.rgb[idx_seen] * (1 - alpha) + palette[i % len(palette)] * alpha\n",
    "\n",
    "# Prepare ScanNet colors and class names for visualization\n",
    "class_colors = list(SCANNET_COLOR_MAP.values())\n",
    "class_names = CLASS_LABELS\n",
    "\n",
    "visualize_mm_data(mm_data_less, class_names=class_names, class_colors=class_colors, figsize=1000, pointsize=3, voxel=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiddling with poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def rgb_to_plotly_rgb(rgb):\n",
    "    \"\"\"Convert torch.Tensor of float RGB values in [0, 1] to\n",
    "    plotly-friendly RGB format.\n",
    "    \"\"\"\n",
    "    assert isinstance(rgb, torch.Tensor) and rgb.max() <= 1.0 and rgb.dim() <= 2\n",
    "\n",
    "    if rgb.dim() == 1:\n",
    "        rgb = rgb.unsqueeze(0)\n",
    "\n",
    "    return [f\"rgb{tuple(x)}\" for x in (rgb * 255).int().numpy()]\n",
    "\n",
    "def hex_to_tensor(h):\n",
    "    h = h.lstrip('#')\n",
    "    rgb = tuple(int(h[i:i+2], 16) for i in (0, 2, 4))\n",
    "    return torch.Tensor(rgb) / 255\n",
    "\n",
    "def plot3d(data, image_xyz=None):\n",
    "    # Prepare figure\n",
    "    width = 600\n",
    "    height = 400\n",
    "    margin = int(0.02 * min(width, height))\n",
    "    pointsize = 3\n",
    "    layout = go.Layout(\n",
    "        width=width,\n",
    "        height=height,\n",
    "        scene=dict(aspectmode='data', ),  # preserve aspect ratio\n",
    "        margin=dict(l=margin, r=margin, b=margin, t=margin),\n",
    "        uirevision=True)\n",
    "    fig = go.Figure(layout=layout)\n",
    "    initialized_visibility = False\n",
    "\n",
    "    # Draw a trace for RGB 3D point cloud\n",
    "    if getattr(data, 'rgb', None) is not None:\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                name='RGB',\n",
    "                x=data.pos[:, 0],\n",
    "                y=data.pos[:, 1],\n",
    "                z=data.pos[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=pointsize,\n",
    "                    color=rgb_to_plotly_rgb(data.rgb), ),\n",
    "                hoverinfo='x+y+z',\n",
    "                showlegend=False,\n",
    "                visible=True, ))\n",
    "        \n",
    "    if image_xyz is None:\n",
    "        fig.show(config={'displayModeBar': False})\n",
    "        return\n",
    "    \n",
    "    if len(image_xyz.shape) == 1:\n",
    "        image_xyz = image_xyz.reshape((1, -1))\n",
    "    for i, xyz in enumerate(image_xyz):\n",
    "        # Draw image position as ball\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                name=f\"Image {i}\",\n",
    "                x=[xyz[0]],\n",
    "                y=[xyz[1]],\n",
    "                z=[xyz[2]],\n",
    "                mode='markers+text',\n",
    "                marker=dict(\n",
    "                    line_width=2,\n",
    "                    size=pointsize + 4, \n",
    "                    color=plotly.colors.qualitative.Plotly[i], ),\n",
    "                text=f\"<b>{i}</b>\",\n",
    "                textposition=\"bottom center\",\n",
    "                textfont=dict(\n",
    "                    size=16),\n",
    "                hoverinfo='x+y+z+name',\n",
    "                showlegend=False,\n",
    "                visible=True, ))\n",
    "        \n",
    "\n",
    "    fig.show(config={'displayModeBar': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, all_slices = torch.load('/media/drobert-admin/DATA2/datasets/scannet/testing/scannet-sparse/processed/preprocessed_3d_train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "data = all_data.__class__(\n",
    "    pos=all_data.pos[all_slices['pos'][idx]: all_slices['pos'][idx+1]], \n",
    "    rgb=all_data.rgb[all_slices['rgb'][idx]: all_slices['rgb'][idx+1]], \n",
    "    y=all_data.y[all_slices['y'][idx]: all_slices['y'][idx+1]])\n",
    "\n",
    "from torch_points3d.core.data_transform.grid_transform import GridSampling3D\n",
    "data = GridSampling3D(0.1)(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3d(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.segmentation.multimodal.scannet import load_pose, read_image_pose_pairs\n",
    "import os\n",
    "import os.path as osp\n",
    "import glob\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "scannet_dir = '/media/drobert-admin/DATA2/datasets/scannet/testing/scannet-sparse/raw/scans'\n",
    "scan_name = 'scene0000_00'\n",
    "scan_sens_dir = osp.join(scannet_dir, scan_name, 'sens')\n",
    "\n",
    "image_info_list = [\n",
    "    {'path': i_file, 'extrinsic': load_pose(p_file)}\n",
    "    for i_file, p_file in read_image_pose_pairs(\n",
    "        osp.join(scan_sens_dir, 'color'),\n",
    "        osp.join(scan_sens_dir, 'pose'),\n",
    "        image_suffix='.jpg', pose_suffix='.txt', verbose=True)]\n",
    "\n",
    "intrinsic = load_pose(osp.join(scan_sens_dir, 'intrinsic/intrinsic_color.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# create camera intrinsics\n",
    "def make_intrinsic(fx, fy, mx, my):\n",
    "    intrinsic = np.eye(4)\n",
    "    intrinsic[0][0] = fx\n",
    "    intrinsic[1][1] = fy\n",
    "    intrinsic[0][2] = mx\n",
    "    intrinsic[1][2] = my\n",
    "    return intrinsic\n",
    "\n",
    "\n",
    "# create camera intrinsics\n",
    "def adjust_intrinsic(intrinsic, intrinsic_image_dim, image_dim):\n",
    "    if intrinsic_image_dim == image_dim:\n",
    "        return intrinsic\n",
    "    resize_width = int(math.floor(image_dim[1] * float(intrinsic_image_dim[0]) / float(intrinsic_image_dim[1])))\n",
    "    intrinsic[0, 0] *= float(resize_width) / float(intrinsic_image_dim[0])\n",
    "    intrinsic[1, 1] *= float(image_dim[1]) / float(intrinsic_image_dim[1])\n",
    "    # account for cropping here\n",
    "    intrinsic[0, 2] *= float(image_dim[0] - 1) / float(intrinsic_image_dim[0] - 1)\n",
    "    intrinsic[1, 2] *= float(image_dim[1] - 1) / float(intrinsic_image_dim[1] - 1)\n",
    "    return intrinsic\n",
    "\n",
    "\n",
    "class LinkCreator(object):\n",
    "    def __init__(self, intrinsic, image_dim=(320, 240), voxelSize=0.05):\n",
    "#         self.intrinsic = make_intrinsic(fx=fx, fy=fy, mx=mx, my=my)\n",
    "        self.intrinsic = intrinsic\n",
    "        self.intrinsic = adjust_intrinsic(self.intrinsic, intrinsic_image_dim=[640, 480], image_dim=image_dim)\n",
    "        self.imageDim = image_dim\n",
    "        self.voxel_size = voxelSize\n",
    "\n",
    "    def computeLinking(self, camera_to_world, coords):\n",
    "        \"\"\"\n",
    "        :param camera_to_world: 4 x 4\n",
    "        :param coords: N x 3 format\n",
    "        :param depth: H x W format\n",
    "        :return: linking, N x 3 format, (H,W,mask)\n",
    "        \"\"\"\n",
    "#         link = torch.zeros((3, coords.shape[0]), dtype=np.int)\n",
    "        out = torch.zeros((3, coords.shape[0]), dtype=torch.long)\n",
    "        coordsNew = torch.cat([coords, torch.ones([coords.shape[0], 1])], dim=1).T\n",
    "        assert coordsNew.shape[0] == 4, \"[!] Shape error\"\n",
    "\n",
    "        world_to_camera = torch.inverse(camera_to_world)\n",
    "        p = world_to_camera.mm(coordsNew)\n",
    "        p[0] = (p[0] * self.intrinsic[0][0]) / p[2] + self.intrinsic[0][2]\n",
    "        p[1] = (p[1] * self.intrinsic[1][1]) / p[2] + self.intrinsic[1][2]\n",
    "        pi = torch.round(p).long()\n",
    "        inside_mask = (pi[0] >= 0) * (pi[1] >= 0) * (pi[0] < self.imageDim[0]) * (pi[1] < self.imageDim[1]) * (p[2] > 0)\n",
    "        \n",
    "        # TODO : which one is it ?\n",
    "        out[0][inside_mask] = pi[0][inside_mask]\n",
    "        out[1][inside_mask] = pi[1][inside_mask]\n",
    "        out[2][inside_mask] = 1\n",
    "        \n",
    "        return out.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to find image centers\n",
    "image_xyz = []\n",
    "for i in range(len(image_info_list)):\n",
    "    \n",
    "    P = image_info_list[i]['extrinsic']\n",
    "    R = P[:3, :3]\n",
    "    T = P[:3, 3]\n",
    "    C = -R.T.mm(T.view(-1, 1))\n",
    "    image_xyz.append(C.view(1, -1))\n",
    "image_xyz = torch.cat(image_xyz, dim=0)\n",
    "\n",
    "plot3d(data, image_xyz=image_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to find image centers\n",
    "image_xyz = torch.cat([image_info_list[i]['extrinsic'][:3, [3]].T for i in range(len(image_info_list))], dim=0)\n",
    "\n",
    "plot3d(data, image_xyz=image_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to find image centers\n",
    "axis_align_matrix = torch.Tensor([0.945519, 0.325568, 0.000000, -5.384390, -0.325568, 0.945519, 0.000000, -2.871780, 0.000000, 0.000000, 1.000000, -0.064350, 0.000000, 0.000000, 0.000000, 1.000000]).reshape((4, 4))\n",
    "\n",
    "xyzh = torch.cat([image_info_list[i]['extrinsic'][:, [3]].T for i in range(len(image_info_list))], dim=0)\n",
    "# xyzh = torch.ones_like(image_xyz)\n",
    "# xyzh[:, :3] = image_xyz\n",
    "xyzh = xyzh.mm(axis_align_matrix.T)\n",
    "image_xyz = xyzh[:, :3]\n",
    "\n",
    "plot3d(data, image_xyz=image_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "LC = LinkCreator(load_pose(osp.join(scan_sens_dir, 'intrinsic/intrinsic_color.txt')), image_dim=(1296, 968), voxelSize=0.05)\n",
    "link = LC.computeLinking(image_info_list[idx]['extrinsic'], data.pos)\n",
    "\n",
    "data_v = data.clone()\n",
    "data_v.rgb[link[:, 2] == 1] = data_v.rgb[link[:, 2] == 1] * 0.5 + torch.Tensor([0.5, 0, 0])\n",
    "\n",
    "plot3d(data_v, image_xyz=image_xyz[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "data = all_data.__class__(\n",
    "    pos=all_data.pos[all_slices['pos'][idx]: all_slices['pos'][idx+1]], \n",
    "    rgb=all_data.rgb[all_slices['rgb'][idx]: all_slices['rgb'][idx+1]], \n",
    "    y=all_data.y[all_slices['y'][idx]: all_slices['y'][idx+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_align_matrix = torch.Tensor([0.945519, 0.325568, 0.000000, -5.384390, -0.325568, 0.945519, 0.000000, -2.871780, 0.000000, 0.000000, 1.000000, -0.064350, 0.000000, 0.000000, 0.000000, 1.000000]).reshape((4, 4))\n",
    "\n",
    "indices = [0, 5, 10, 20, 30]\n",
    "palette = [hex_to_tensor(c) for c in plotly.colors.qualitative.Plotly]\n",
    "rgb_size = (1296, 968)\n",
    "proj_size = (320, 240)\n",
    "\n",
    "intrinsic = load_pose(osp.join(scan_sens_dir, 'intrinsic/intrinsic_depth.txt'))\n",
    "LC = LinkCreator(intrinsic, image_dim=proj_size)\n",
    "\n",
    "image_xyz_list = []\n",
    "image_target_list = []\n",
    "image_proj_list = []\n",
    "data_v = data.clone()\n",
    "alpha = 0.5\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    \n",
    "    # Compute projection\n",
    "    extrinsic = axis_align_matrix.mm(image_info_list[idx]['extrinsic'])\n",
    "    link = LC.computeLinking(extrinsic, data.pos)\n",
    "    \n",
    "    # Compute\n",
    "    color = palette[i % len(palette)]\n",
    "    data_v.rgb[link[:, 2] == 1] = (data_v.rgb[link[:, 2] == 1]) * (1 - alpha) + color * alpha\n",
    "    \n",
    "    image_xyz_list.append(extrinsic[:3, 3].view(1, -1))\n",
    "\n",
    "    # Recover target RGB\n",
    "    image_target = np.asarray(Image.open(image_info_list[idx]['path']).resize(proj_size))\n",
    "    image_target_list.append(image_target)\n",
    "\n",
    "    # Compute artificial RGB image from projected points\n",
    "    mask = link[:, 2] == 1\n",
    "    image_proj = torch.zeros((*proj_size, 3)).byte()\n",
    "    image_proj[link[mask, 0], link[mask, 1]] = (data.rgb[mask] * 255).byte()\n",
    "    image_proj_list.append(image_proj)\n",
    "\n",
    "plot3d(data_v, image_xyz=torch.cat(image_xyz_list, dim=0))\n",
    "\n",
    "# Draw images\n",
    "for i, (image_target, image_proj) in enumerate(zip(image_target_list, image_proj_list)):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,3.5))\n",
    "    fig.suptitle(f\"Image {i}\")\n",
    "    ax1.imshow(image_target)\n",
    "    ax2.imshow(image_proj.transpose(0, 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle intrinsics and extrinsics in SameSettingImageData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_GPU = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "from matplotlib.colors import ListedColormap\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "os.system('/usr/bin/Xvfb :99 -screen 0 1024x768x24 &')\n",
    "os.environ['DISPLAY'] = ':99'\n",
    "os.environ['PYVISTA_OFF_SCREEN'] = 'True'\n",
    "os.environ['PYVISTA_USE_PANEL'] = 'True'\n",
    "torch.cuda.set_device(I_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = os.path.dirname(os.getcwd())\n",
    "ROOT = os.path.join(DIR, \"..\")\n",
    "sys.path.insert(0, ROOT)\n",
    "sys.path.insert(0, DIR)\n",
    "# from torch_points3d.datasets.segmentation.scannet import ScannetDataset\n",
    "from torch_points3d.datasets.segmentation.multimodal.scannet import ScannetDatasetMM\n",
    "from torch_points3d.datasets.segmentation.multimodal import IGNORE_LABEL\n",
    "\n",
    "from torch_points3d.visualization.multimodal_data import visualize_mm_data, hex_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.segmentation.multimodal.scannet import load_pose, read_image_pose_pairs\n",
    "from torch_points3d.core.data_transform.grid_transform import GridSampling3D\n",
    "import os\n",
    "import os.path as osp\n",
    "import glob\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "scannet_dir = '/media/drobert-admin/DATA2/datasets/scannet/testing/scannet-sparse/raw/scans'\n",
    "scan_name = 'scene0000_00'\n",
    "scan_sens_dir = osp.join(scannet_dir, scan_name, 'sens')\n",
    "\n",
    "all_data, all_slices = torch.load('/media/drobert-admin/DATA2/datasets/scannet/testing/scannet-sparse/processed/preprocessed_3d_train.pt')\n",
    "\n",
    "idx = 0\n",
    "data = all_data.__class__(\n",
    "    pos=all_data.pos[all_slices['pos'][idx]: all_slices['pos'][idx+1]], \n",
    "    rgb=all_data.rgb[all_slices['rgb'][idx]: all_slices['rgb'][idx+1]], \n",
    "    y=all_data.y[all_slices['y'][idx]: all_slices['y'][idx+1]])\n",
    "\n",
    "\n",
    "n_full = data.num_nodes\n",
    "data = GridSampling3D(0.05)(data)\n",
    "n_sampled = data.num_nodes\n",
    "print(f'{n_sampled / n_full * 100:0.1f}% subsampled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_points3d.datasets.segmentation.scannet import read_axis_align_matrix\n",
    "\n",
    "def read_axis_align_matrix(filename):\n",
    "    lines = open(filename).readlines()\n",
    "    axis_align_matrix = None\n",
    "    for line in lines:\n",
    "        if \"axisAlignment\" in line:\n",
    "            axis_align_matrix = torch.Tensor([float(x) for x in line.rstrip().strip(\"axisAlignment = \").split(\" \")]).reshape((4, 4))\n",
    "            break\n",
    "    return axis_align_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_sens_dir = osp.join(scannet_dir, scan_name, 'sens')\n",
    "meta_file = osp.join(scannet_dir, scan_name, scan_name + \".txt\")\n",
    "    \n",
    "image_info_list = [\n",
    "    {'path': i_file, 'extrinsic': load_pose(p_file)}\n",
    "    for i_file, p_file in read_image_pose_pairs(\n",
    "        osp.join(scan_sens_dir, 'color'),\n",
    "        osp.join(scan_sens_dir, 'pose'),\n",
    "        image_suffix='.jpg', pose_suffix='.txt')]\n",
    "\n",
    "# Aggregate all RGB image paths\n",
    "path = np.array([info['path'] for info in image_info_list])\n",
    "\n",
    "# Aggregate all extrinsic 4x4 matrices\n",
    "# Train and val scans have undergone axis alignment\n",
    "# transformations. Need to recover and apply those\n",
    "# to camera poses too. Test scans have no axis\n",
    "# alignment\n",
    "axis_align_matrix = read_axis_align_matrix(meta_file)\n",
    "if axis_align_matrix is not None:\n",
    "    extrinsic = torch.cat([\n",
    "        axis_align_matrix.mm(info['extrinsic']).unsqueeze(0)\n",
    "        for info in image_info_list], dim=0)\n",
    "else:\n",
    "    extrinsic = torch.cat([\n",
    "        info['extrinsic'].unsqueeze(0)\n",
    "        for info in image_info_list], dim=0)\n",
    "\n",
    "# For easier image handling, extract the images\n",
    "# position from the extrinsic matrices\n",
    "xyz = extrinsic[:, :3, 3]\n",
    "\n",
    "# Read intrinsic parameters for the depth camera\n",
    "# because this is the one related to the extrinsic.\n",
    "# Strangely, using the color camera intrinsic along\n",
    "# with the pose does not produce the expected\n",
    "# projection\n",
    "intrinsic = load_pose(osp.join(scan_sens_dir, 'intrinsic/intrinsic_depth.txt'))\n",
    "fx = intrinsic[0][0].repeat(len(image_info_list))\n",
    "fy = intrinsic[1][1].repeat(len(image_info_list))\n",
    "mx = intrinsic[0][2].repeat(len(image_info_list))\n",
    "my = intrinsic[1][2].repeat(len(image_info_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.multimodal.image import SameSettingImageData\n",
    "\n",
    "DEPTH_IMG_SIZE = (640, 480)\n",
    "img_size = (320, 240)\n",
    "image_data = SameSettingImageData(ref_size=DEPTH_IMG_SIZE, path=path, pos=xyz, fx=fx, fy=fy, mx=mx, my=my, proj_upscale=1, extrinsic=extrinsic)\n",
    "image_data.ref_size = img_size\n",
    "image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.data_transform.multimodal.image import MapImages\n",
    "from torch_points3d.core.multimodal.data import MMData\n",
    "from torch_points3d.core.multimodal.image import ImageData\n",
    "\n",
    "transform = MapImages(method='SplattingVisibility', r_max=8, r_min=0.5, equirectangular=False, voxel=0.05, k_swell=1.0, d_swell=1000, exact=True)\n",
    "\n",
    "data.mapping_index = torch.arange(data.num_nodes).long()\n",
    "data_out, image_data_out = transform(data, image_data)\n",
    "\n",
    "mm_data = MMData(data_out, image=ImageData([image_data_out]))\n",
    "mm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mm_data(mm_data, figsize=1000, pointsize=3, voxel=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.segmentation.scannet import VALID_CLASS_IDS, SCANNET_COLOR_MAP, CLASS_LABELS, NUM_CLASSES, IGNORE_LABEL\n",
    "\n",
    "def remap_scannet_labels(semantic_label, valid_class_idx=VALID_CLASS_IDS, donotcare_class_ids=[]):\n",
    "    \"\"\"Remaps labels to [0 ; num_labels -1].\"\"\"\n",
    "    new_labels = semantic_label.clone()\n",
    "    mapping_dict = {idx: i for i, idx in enumerate(valid_class_idx)}\n",
    "    for idx in range(NUM_CLASSES):\n",
    "        if idx not in mapping_dict:\n",
    "            mapping_dict[idx] = IGNORE_LABEL\n",
    "    for idx in donotcare_class_ids:\n",
    "        mapping_dict[idx] = IGNORE_LABEL\n",
    "    for source, target in mapping_dict.items():\n",
    "        mask = semantic_label == source\n",
    "        new_labels[mask] = target\n",
    "\n",
    "    broken_labels = new_labels >= len(valid_class_idx)\n",
    "    new_labels[broken_labels] = IGNORE_LABEL\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MMData object, keep only a few images, remap labels and remove \n",
    "# IGNORE_LABEL points\n",
    "mm_data_less = MMData(data_out.clone(), image=ImageData([image_data_out.clone()[:5]]))\n",
    "mm_data_less.data.y = remap_scannet_labels(mm_data_less.data.y)\n",
    "mm_data_less = mm_data_less[mm_data_less.data.y != IGNORE_LABEL]\n",
    "\n",
    "# Prepare ScanNet colors and class names for visualization\n",
    "class_colors = list(SCANNET_COLOR_MAP.values())\n",
    "class_names = CLASS_LABELS\n",
    "\n",
    "visualize_mm_data(mm_data_less, class_names=class_names, class_colors=class_colors, front='y', figsize=1000, pointsize=3, voxel=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "\n",
    "# image_idx = [20, 22, 24, 49]\n",
    "image_idx = [20, 24, 49]\n",
    "alpha = 0.5\n",
    "palette = [hex_to_tensor(c) for c in plotly.colors.qualitative.Plotly]\n",
    "\n",
    "# Create MMData object, keep only a few images, remap labels and remove \n",
    "# IGNORE_LABEL points\n",
    "mm_data_less = MMData(data_out.clone(), image=ImageData([image_data_out.clone()[image_idx]]))\n",
    "mm_data_less.data.y = remap_scannet_labels(mm_data_less.data.y)\n",
    "mm_data_less = mm_data_less[mm_data_less.data.y != IGNORE_LABEL]\n",
    "\n",
    "# Color the points wrt the images that see them, following the plotly\n",
    "# palette used to color image balls\n",
    "for i in range(len(image_idx)):\n",
    "    mapping = mm_data_less.modalities['image'][0].mappings.select_images(i)\n",
    "    idx_seen = mapping.pointers[1:] > mapping.pointers[:-1]\n",
    "    mm_data_less.data.rgb[idx_seen] = mm_data_less.data.rgb[idx_seen] * (1 - alpha) + palette[i % len(palette)] * alpha\n",
    "\n",
    "# Prepare ScanNet colors and class names for visualization\n",
    "class_colors = list(SCANNET_COLOR_MAP.values())\n",
    "class_names = CLASS_LABELS\n",
    "\n",
    "visualize_mm_data(mm_data_less, class_names=class_names, class_colors=class_colors, figsize=1000, pointsize=3, voxel=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_GPU = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "from matplotlib.colors import ListedColormap\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "os.system('/usr/bin/Xvfb :99 -screen 0 1024x768x24 &')\n",
    "os.environ['DISPLAY'] = ':99'\n",
    "os.environ['PYVISTA_OFF_SCREEN'] = 'True'\n",
    "os.environ['PYVISTA_USE_PANEL'] = 'True'\n",
    "torch.cuda.set_device(I_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = os.path.dirname(os.getcwd())\n",
    "ROOT = os.path.join(DIR, \"..\")\n",
    "sys.path.insert(0, ROOT)\n",
    "sys.path.insert(0, DIR)\n",
    "# from torch_points3d.datasets.segmentation.scannet import ScannetDataset\n",
    "from torch_points3d.datasets.segmentation.multimodal.scannet import ScannetDatasetMM\n",
    "from torch_points3d.datasets.segmentation.multimodal import IGNORE_LABEL\n",
    "\n",
    "from torch_points3d.visualization.multimodal_data import visualize_mm_data, hex_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.segmentation.multimodal.scannet import load_pose, read_image_pose_pairs\n",
    "from torch_points3d.core.data_transform.grid_transform import GridSampling3D\n",
    "import os\n",
    "import os.path as osp\n",
    "import glob\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "scannet_dir = '/media/drobert-admin/DATA2/datasets/scannet/testing/scannet-sparse/raw/scans'\n",
    "scan_name = 'scene0000_00'\n",
    "scan_sens_dir = osp.join(scannet_dir, scan_name, 'sens')\n",
    "\n",
    "all_data, all_slices = torch.load('/media/drobert-admin/DATA2/datasets/scannet/testing/scannet-sparse/processed/preprocessed_3d_train.pt')\n",
    "\n",
    "idx = 0\n",
    "data = all_data.__class__(\n",
    "    pos=all_data.pos[all_slices['pos'][idx]: all_slices['pos'][idx+1]], \n",
    "    rgb=all_data.rgb[all_slices['rgb'][idx]: all_slices['rgb'][idx+1]], \n",
    "    y=all_data.y[all_slices['y'][idx]: all_slices['y'][idx+1]])\n",
    "\n",
    "n_full = data.num_nodes\n",
    "data = GridSampling3D(0.05)(data)\n",
    "n_sampled = data.num_nodes\n",
    "print(f'{n_sampled / n_full * 100:0.1f}% subsampled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_points3d.datasets.segmentation.scannet import read_axis_align_matrix\n",
    "\n",
    "def read_axis_align_matrix(filename):\n",
    "    lines = open(filename).readlines()\n",
    "    axis_align_matrix = None\n",
    "    for line in lines:\n",
    "        if \"axisAlignment\" in line:\n",
    "            axis_align_matrix = torch.Tensor([float(x) for x in line.rstrip().strip(\"axisAlignment = \").split(\" \")]).reshape((4, 4))\n",
    "            break\n",
    "    return axis_align_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_sens_dir = osp.join(scannet_dir, scan_name, 'sens')\n",
    "meta_file = osp.join(scannet_dir, scan_name, scan_name + \".txt\")\n",
    "    \n",
    "image_info_list = [\n",
    "    {'path': i_file, 'extrinsic': load_pose(p_file)}\n",
    "    for i_file, p_file in read_image_pose_pairs(\n",
    "        osp.join(scan_sens_dir, 'color'),\n",
    "        osp.join(scan_sens_dir, 'pose'),\n",
    "        image_suffix='.jpg', pose_suffix='.txt')]\n",
    "\n",
    "# Aggregate all RGB image paths\n",
    "path = np.array([info['path'] for info in image_info_list])\n",
    "\n",
    "# Aggregate all extrinsic 4x4 matrices\n",
    "# Train and val scans have undergone axis alignment\n",
    "# transformations. Need to recover and apply those\n",
    "# to camera poses too. Test scans have no axis\n",
    "# alignment\n",
    "axis_align_matrix = read_axis_align_matrix(meta_file)\n",
    "if axis_align_matrix is not None:\n",
    "    extrinsic = torch.cat([\n",
    "        axis_align_matrix.mm(info['extrinsic']).unsqueeze(0)\n",
    "        for info in image_info_list], dim=0)\n",
    "else:\n",
    "    extrinsic = torch.cat([\n",
    "        info['extrinsic'].unsqueeze(0)\n",
    "        for info in image_info_list], dim=0)\n",
    "\n",
    "# For easier image handling, extract the images\n",
    "# position from the extrinsic matrices\n",
    "xyz = extrinsic[:, :3, 3]\n",
    "\n",
    "# Read intrinsic parameters for the depth camera\n",
    "# because this is the one related to the extrinsic.\n",
    "# Strangely, using the color camera intrinsic along\n",
    "# with the pose does not produce the expected\n",
    "# projection\n",
    "intrinsic = load_pose(osp.join(scan_sens_dir, 'intrinsic/intrinsic_depth.txt'))\n",
    "fx = intrinsic[0][0].repeat(len(image_info_list))\n",
    "fy = intrinsic[1][1].repeat(len(image_info_list))\n",
    "mx = intrinsic[0][2].repeat(len(image_info_list))\n",
    "my = intrinsic[1][2].repeat(len(image_info_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.multimodal.image import SameSettingImageData\n",
    "\n",
    "DEPTH_IMG_SIZE = (640, 480)\n",
    "img_size = (320, 240)\n",
    "image_data = SameSettingImageData(ref_size=DEPTH_IMG_SIZE, path=path, pos=xyz, fx=fx, fy=fy, mx=mx, my=my, proj_upscale=1, extrinsic=extrinsic)\n",
    "image_data.ref_size = img_size\n",
    "image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.data_transform.multimodal.image import MapImages\n",
    "from torch_points3d.core.multimodal.data import MMData\n",
    "from torch_points3d.core.multimodal.image import ImageData\n",
    "\n",
    "transform = MapImages(method='SplattingVisibility', r_max=8, r_min=0.5, equirectangular=False, voxel=0.05, k_swell=1.0, d_swell=1000, exact=True)\n",
    "\n",
    "data.mapping_index = torch.arange(data.num_nodes).long()\n",
    "data_out, image_data_out = transform(data, image_data)\n",
    "\n",
    "mm_data = MMData(data_out, image=ImageData([image_data_out]))\n",
    "mm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp3d",
   "language": "python",
   "name": "tp3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
