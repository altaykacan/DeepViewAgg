{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_GPU = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "from matplotlib.colors import ListedColormap\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "os.system('/usr/bin/Xvfb :99 -screen 0 1024x768x24 &')\n",
    "os.environ['DISPLAY'] = ':99'\n",
    "os.environ['PYVISTA_OFF_SCREEN'] = 'True'\n",
    "os.environ['PYVISTA_USE_PANEL'] = 'True'\n",
    "torch.cuda.set_device(I_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = os.path.dirname(os.getcwd())\n",
    "ROOT = os.path.join(DIR, \"..\")\n",
    "sys.path.insert(0, ROOT)\n",
    "sys.path.insert(0, DIR)\n",
    "from torch_points3d.datasets.segmentation.multimodal.s3dis import S3DISFusedDataset\n",
    "from torch_points3d.datasets.segmentation.multimodal import IGNORE_LABEL\n",
    "\n",
    "from torch_points3d.visualization.multimodal_data import visualize_mm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.offline as pyo\n",
    "# pyo.init_notebook_mode()\n",
    "\n",
    "import plotly.io as pio\n",
    "# pio.renderers\n",
    "\n",
    "# For local jupyterlab\n",
    "pio.renderers.default = 'jupyterlab'\n",
    "\n",
    "# Ror remote HPC jupyterhub.\n",
    "# Other working (but seemingly slower) options are: 'sphinx_gallery' and 'iframe'\n",
    "# pio.renderers.default = 'iframe_connected'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Batch\n",
    "from torch import Tensor\n",
    "\n",
    "OBJECT_COLOR = [\n",
    "        [233, 229, 107],  #'ceiling' .-> .yellow\n",
    "        [95, 156, 196],  #'floor' .-> . blue\n",
    "        [179, 116, 81],  #'wall'  ->  brown\n",
    "        [241, 149, 131],  #'beam'  ->  salmon\n",
    "        [81, 163, 148],  #'column'  ->  bluegreen\n",
    "        [77, 174, 84],  #'window'  ->  bright green\n",
    "        [108, 135, 75],  #'door'   ->  dark green\n",
    "        [41, 49, 101],  #'chair'  ->  darkblue\n",
    "        [79, 79, 76],  #'table'  ->  dark grey\n",
    "        [223, 52, 52],  #'bookcase'  ->  red\n",
    "        [89, 47, 95],  #'sofa'  ->  purple\n",
    "        [81, 109, 114],  #'board'   ->  grey\n",
    "        [233, 233, 229],  #'clutter'  ->  light grey\n",
    "        [0, 0, 0],  # unlabelled .->. black\n",
    "    ]\n",
    "\n",
    "CLASSES = [\n",
    "    'ceiling',\n",
    "    'floor',\n",
    "    'wall',\n",
    "    'beam',\n",
    "    'column',\n",
    "    'window',\n",
    "    'door',\n",
    "    'chair',\n",
    "    'table',\n",
    "    'bookcase',\n",
    "    'sofa',\n",
    "    'board',\n",
    "    'clutter',\n",
    "    'unlabelled',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import Center, RandomRotate\n",
    "from torch_points3d.core.data_transform import RandomNoise, RandomScaleAnisotropic, RandomSymmetry, \\\n",
    "    DropFeature, XYZFeature, AddFeatsByKeys\n",
    "from torch_points3d.core.data_transform.multimodal.image import ToFloatImage, AddPixelHeightFeature, \\\n",
    "    PickImagesFromMappingArea, PickImagesFromMemoryCredit, CropImageGroups, Normalize\n",
    "from torch_points3d.datasets.base_dataset import BaseDataset\n",
    "from torch_points3d.datasets.base_dataset_multimodal import BaseDatasetMM\n",
    "\n",
    "def sample_real_data(tg_dataset, idx=0):\n",
    "    \"\"\"\n",
    "    Temporarily remove the 3D and 2D transforms affecting the point \n",
    "    positions and images from the dataset to better visualize points \n",
    "    and images relative positions.\n",
    "    \"\"\"\n",
    "    transform = tg_dataset.transform\n",
    "    tg_dataset.transform = BaseDataset.remove_transform(transform, [Center, RandomNoise,\n",
    "        RandomRotate, RandomScaleAnisotropic, RandomSymmetry, DropFeature, AddFeatsByKeys])\n",
    "    \n",
    "    transform_image = tg_dataset.transform_image\n",
    "#     tg_dataset.transform_image = BaseDatasetMM.remove_multimodal_transform(transform_image, [ToFloatImage, AddPixelHeightFeature,])\n",
    "#     tg_dataset.transform_image = BaseDatasetMM.remove_multimodal_transform(transform_image, [ToFloatImage, PickImagesFromMappingArea, AddPixelHeightFeature, ])\n",
    "#     tg_dataset.transform_image = BaseDatasetMM.remove_multimodal_transform(transform_image, [ToFloatImage, PickImagesFromMappingArea, AddPixelHeightFeature,PickImagesFromMemoryCredit ])\n",
    "    #tg_dataset.transform_image = BaseDatasetMM.remove_multimodal_transform(transform_image, [ToFloatImage, AddPixelHeightFeature,PickImagesFromMemoryCredit ])\n",
    "    tg_dataset.transform_image = BaseDatasetMM.remove_multimodal_transform(transform_image, [ToFloatImage, AddPixelHeightFeature, Normalize ])\n",
    "\n",
    "    out = tg_dataset[idx]\n",
    "    \n",
    "    tg_dataset.transform = transform\n",
    "    tg_dataset.transform_image = transform_image\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from torch_points3d.utils.config import hydra_read\n",
    "\n",
    "# Set root to the DATA drive, where the data was downloaded\n",
    "# DATA_ROOT = '/mnt/fa444ffd-fdb4-4701-88e7-f00297a8e29b/projects/datasets/s3dis'  # ???\n",
    "# DATA_ROOT = '/media/drobert-admin/DATA/datasets/s3dis'  # IGN DATA\n",
    "# DATA_ROOT = '/media/drobert-admin/DATA2/datasets/s3dis'  # IGN DATA2\n",
    "DATA_ROOT = '/var/data/drobert/datasets/s3dis'  # AI4GEO\n",
    "# DATA_ROOT = '/home/qt/robertda/scratch/datasets/s3dis'  # CNES\n",
    "# DATA_ROOT = '/raid/datasets/pointcloud/data/s3dis'  # ENGIE\n",
    "\n",
    "overrides = [\n",
    "    'task=segmentation',\n",
    "#     'data=segmentation/multimodal/s3disfused/no3d_5cm_768x384-exact',\n",
    "    'data=segmentation/multimodal/s3disfused-sparse',\n",
    "    'models=segmentation/multimodal/sparseconv3d',\n",
    "    'model_name=Res16UNet34-L4-early-ade20k-interpolate',\n",
    "    'data.fold=5',\n",
    "    'data.sample_per_epoch=2000',\n",
    "#     f\"data.dataroot={os.path.join(DATA_ROOT, '5cm_exact_1024x512')}\",\n",
    "    f\"data.dataroot={os.path.join(DATA_ROOT, '5cm_exact_512x256')}\",\n",
    "    \n",
    "#     f\"data.dataroot={os.path.join(DATA_ROOT, 'temp')}\",\n",
    "#     'data.resolution_2d=[512, 256]'\n",
    "]\n",
    "\n",
    "cfg = hydra_read(overrides)\n",
    "\n",
    "# print(OmegaConf.to_yaml(cfg))# Load S3DIS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load S3DIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "\n",
    "dataset = S3DISFusedDataset(cfg.data)\n",
    "# print(dataset)\n",
    "\n",
    "print(f\"Time = {time() - start:0.1f} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize multimodal spherical samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove pixel memory credit transform\n",
    "from torch_points3d.core.data_transform.multimodal.image import PickImagesFromMemoryCredit\n",
    "from torch_points3d.datasets.base_dataset_multimodal import BaseDatasetMM\n",
    "for x in [dataset.train_dataset, dataset.val_dataset, dataset.test_dataset[0]]:\n",
    "    x.transform_image = BaseDatasetMM.remove_multimodal_transform(x.transform_image, [PickImagesFromMemoryCredit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get multimodal spherical sample\n",
    "# mm_data = sample_real_data(dataset.test_dataset[0], idx=100)\n",
    "# mm_data = sample_real_data(dataset.train_dataset)\n",
    "# mm_data = sample_real_data(dataset.val_dataset)\n",
    "\n",
    "idx = 215\n",
    "mm_data = sample_real_data(dataset.test_dataset[0], idx=idx)\n",
    "# print(mm_data)\n",
    "\n",
    "# credit_used = sum([np.prod(im.img_size) * im.num_views for im in mm_data.modalities['image']])\n",
    "# credit_max = dataset.train_transform_image.transforms[-1].credit\n",
    "# print(f\"Used pixel credit: {credit_used} / {credit_max} = {credit_used / credit_max * 100:0.1f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mm_data(mm_data, class_names=CLASSES, class_colors=OBJECT_COLOR, figsize=1000, voxel=0.05, show_3d=True, show_2d=True, front='y', alpha=3, pointsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.multimodal.image import ImageData\n",
    "\n",
    "idx = 215\n",
    "\n",
    "# Set the minimum crop size\n",
    "dataset.test_dataset[0].transform_image.transforms[-2].min_size = 128\n",
    "\n",
    "# Sample\n",
    "mm_data = sample_real_data(dataset.test_dataset[0], idx=idx)\n",
    "\n",
    "# Select the points\n",
    "idx_subsphere = torch.where(mm_data.data.pos[:, 1] < -7.6)[0]\n",
    "mm_data_clean = mm_data[idx_subsphere]\n",
    "\n",
    "# Select the images\n",
    "# mm_data_clean.modalities['image'] = ImageData([\n",
    "#     mm_data_clean.modalities['image'][2][1],\n",
    "#     mm_data_clean.modalities['image'][1],\n",
    "#     mm_data_clean.modalities['image'][-2][0],\n",
    "#     mm_data_clean.modalities['image'][-1]\n",
    "# ])\n",
    "mm_data_clean.modalities['image'] = ImageData([\n",
    "    mm_data_clean.modalities['image'][0][1],\n",
    "    mm_data_clean.modalities['image'][1][1],\n",
    "    mm_data_clean.modalities['image'][2]\n",
    "])\n",
    "\n",
    "# Set all but object of interest to label 0\n",
    "# mask_object = mm_data_clean.data.y == 11  # board\n",
    "# mask_object = torch.logical_and(mm_data_clean.data.pos[:, 1] > -9.3, mm_data_clean.data.y == 7)  # chair in the middle\n",
    "\n",
    "# # a = torch.where(mask_object)[0]\n",
    "# # a[torch.randint(low=0, high=a.shape[0], size=(1,))]\n",
    "# # mask_object = torch.zeros_like(mask_object)\n",
    "# # mask_object[a[torch.randint(low=0, high=a.shape[0], size=(1,))]] = True\n",
    "\n",
    "# mm_data_clean.y[~mask_object] = 0\n",
    "\n",
    "mm_data_clean.y[mm_data_clean.y == 0] = 8\n",
    "\n",
    "# # Keep only mappings to the object of interest\n",
    "# idx_object = torch.where(mask_object)[0]\n",
    "# for im in mm_data_clean.modalities['image']:\n",
    "#     im.mappings = im.mappings[idx_object]\n",
    "#     im.mappings = im.mappings.reindex_groups(idx_object, num_groups=mask_object.shape[0])\n",
    "\n",
    "visualize_mm_data(mm_data_clean, class_names=CLASSES, class_colors=OBJECT_COLOR, width=1000, height=750, voxel=0.05, show_3d=True, show_2d=False, color_mode='ligth', alpha=2, pointsize=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Downscale 3D\n",
    "# from torch_points3d.core.data_transform import GridSampling3D\n",
    "# from torch_points3d.core.data_transform.multimodal.image import SelectMappingFromPointId\n",
    "\n",
    "# print(mm_data.data.mapping_index)\n",
    "# mm_data.data = GridSampling3D(0.2)(mm_data.data.clone())\n",
    "# print(mm_data.data.mapping_index)\n",
    "\n",
    "# # Subsample the mappings accordingly\n",
    "# mm_data.data , mm_data.images = SelectMappingFromPointId()(mm_data.data, mm_data.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Downscale 3D\n",
    "# from torch_points3d.core.data_transform import GridSampling3D\n",
    "# from torch_points3d.core.data_transform.multimodal.image import SelectMappingFromPointId\n",
    "\n",
    "# print(mm_data.data.mapping_index)\n",
    "# mm_data.data = GridSampling3D(0.2)(mm_data.data.clone())\n",
    "# print(mm_data.data.mapping_index)\n",
    "\n",
    "# # Subsample the mappings accordingly\n",
    "# mm_data.data , mm_data.images = SelectMappingFromPointId()(mm_data.data, mm_data.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Downscale 3D with 'merge' mode\n",
    "# from torch_points3d.core.data_transform import GridSampling3D\n",
    "# from torch_cluster import grid_cluster\n",
    "# from torch_geometric.nn import voxel_grid\n",
    "# from torch_geometric.nn.pool.consecutive import consecutive_cluster\n",
    "\n",
    "# voxel = 0.1\n",
    "\n",
    "# # Get the cluster indices for grid sampling\n",
    "# coords = torch.round((mm_data.data.pos) / voxel)\n",
    "# if \"batch\" not in mm_data.data:\n",
    "#     cluster = grid_cluster(coords, torch.tensor([1, 1, 1]))\n",
    "# else:\n",
    "#     cluster = voxel_grid(coords, mm_data.data.batch, 1)\n",
    "# cluster, _ = consecutive_cluster(cluster)\n",
    "\n",
    "# # Actual 3D sampling\n",
    "# mm_data.data = GridSampling3D(voxel)(mm_data.data.clone())\n",
    "\n",
    "# # Subsample the mappings accordingly with 'merge' mocde\n",
    "# for im in mm_data.images:\n",
    "#     im.mappings = im.mappings.select_points(cluster, merge=True)\n",
    "    \n",
    "# # Reset the mapping_index\n",
    "# # Remark : unlike SelectMappingFromPointId, we don't need to search for\n",
    "# # potentially unseen images when 'merge=True', because the subsampling\n",
    "# # index implies that all points are merged into a new one, so no \n",
    "# # mappings should be lost in the process.\n",
    "# mm_data.data.mapping_index = torch.arange(mm_data.data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Downscale 2D\n",
    "# for im in mm_data.images:\n",
    "#     im = im.update_features_and_scale(im.images[:, :, ::4, ::4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multimodal batch\n",
    "# from torch_points3d.datasets.multimodal.data import MMBatch\n",
    "# mm_data = MMBatch.from_mm_data_list([sample_real_data(dataset.test_dataset[0], idx=i) for i in [0, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_mm_data(mm_data, class_names=CLASSES, class_colors=OBJECT_COLOR, figsize=1000, voxel=0.05, show_3d=True, show_2d=True, color_mode='y', alpha=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Export to HTML for each image coloring mode\n",
    "# for mode in ['light', 'rgb', 'pos', 'y']:\n",
    "#     visualize_mm_data(mm_data, class_names=CLASSES, class_colors=OBJECT_COLOR, figsize=1000, voxel=0.1, show_3d=True, \n",
    "#         show_2d=True, alpha=3, color_mode=mode, path=f'/home/ign.fr/drobert-admin/Bureau/s3dis_visualizations/demo_mm_data/sub_2d__{mode}.html',\n",
    "#         title=f'2D downsampled multimodal sample - Mode={mode}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize whole areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.multimodal.data import MMData\n",
    "\n",
    "# mm_data_large = MMData(dataset.train_dataset._datas[1], dataset.train_dataset._images[1], dataset.train_dataset._mappings[1])\n",
    "mm_data_large = MMData(dataset.test_dataset[0]._datas[0], dataset.test_dataset[0]._images[0], dataset.test_dataset[0]._mappings[0])\n",
    "visualize_mm_data(mm_data_large, class_names=CLASSES, class_colors=OBJECT_COLOR, figsize=1600, voxel=0.05, show_2d=False, max_points=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test multimodal batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.multimodal.data import MMData, MMBatch \n",
    "\n",
    "# Get multimodal spherical sample\n",
    "# mm_data = sample_real_data(dataset.test_dataset[0], idx=50)\n",
    "mm_data_1 = sample_real_data(dataset.train_dataset)\n",
    "mm_data_2 = sample_real_data(dataset.train_dataset)\n",
    "mm_data_3 = sample_real_data(dataset.train_dataset)\n",
    "\n",
    "mm_data_1.images.num_views, mm_data_2.images.num_views, mm_data_3.images.num_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing masks to bypass a local version bug\n",
    "mm_data_1.images.mask = mm_data_2.images.mask = mm_data_3.images.mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batch\n",
    "mm_batch = MMBatch.from_mm_data_list([mm_data_1, mm_data_2, mm_data_3])\n",
    "image_batch = mm_batch.images.read_images(size=mm_batch.images.map_size_low)\n",
    "visualize_mm_data(mm_batch, class_names=CLASSES, class_colors=OBJECT_COLOR, image_batch=image_batch, figsize=800, voxel=0.05, show_3d=True, show_2d=True, color_mode='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sampling #3\n",
    "image_batch_3 = mm_data_3.images.read_images(size=mm_data_3.images.map_size_low)\n",
    "visualize_mm_data(mm_data_3, class_names=CLASSES, class_colors=OBJECT_COLOR, image_batch=image_batch_3, figsize=800, voxel=0.05, show_3d=True, show_2d=True, color_mode='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sampling #3 after batch unpacking\n",
    "mm_data_unpacked_1, mm_data_unpacked_2, mm_data_unpacked_3 = mm_batch.to_mm_data_list()\n",
    "image_batch_3 = mm_data_unpacked_3.images.read_images(size=mm_data_unpacked_3.images.map_size_low)\n",
    "visualize_mm_data(mm_data_unpacked_3, class_names=CLASSES, class_colors=OBJECT_COLOR, image_batch=image_batch_3, figsize=800, voxel=0.05, show_3d=True, show_2d=True, color_mode='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array_equal(mm_data_3.mappings.values[1].values[0], mm_data_unpacked_3.mappings.values[1].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize 3D resampling of multimodal data\n",
    "Under the hood, this calls the indexing mechanism of mappings\n",
    "\n",
    "Remark: this does not make use of `ImageMapping.select_points`. If we wanted to test the `merge` mechanism of `select_points`, we would need to work with a remapping of the points, much like a voxel sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get multimodal spherical sample\n",
    "# mm_data = sample_real_data(dataset.test_dataset[0], idx=50)\n",
    "mm_data = sample_real_data(dataset.train_dataset)\n",
    "print(mm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "image_batch = mm_data.images.read_images(size=mm_data.images.map_size_low)\n",
    "\n",
    "# Visualize\n",
    "visualize_mm_data(mm_data, class_names=CLASSES, class_colors=OBJECT_COLOR, image_batch=image_batch, figsize=1000, voxel=0.05, show_3d=True, show_2d=True, color_mode='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the multimodal data with random indices\n",
    "idx = torch.randint(low=0, high=mm_data.data.num_nodes, size=(int(mm_data.data.num_nodes/4),))\n",
    "mm_data_sliced = mm_data[idx]\n",
    "\n",
    "# Load images (could have been skipped had we used MMData.load)\n",
    "image_batch_sliced = mm_data_sliced.images.read_images(size=mm_data_sliced.images.map_size_low)\n",
    "\n",
    "# Visualize\n",
    "visualize_mm_data(mm_data_sliced, class_names=CLASSES, class_colors=OBJECT_COLOR, image_batch=image_batch_sliced, figsize=1000, voxel=0.05, show_3d=True, show_2d=True, color_mode='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize 2D resampling of multimodal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get multimodal spherical sample\n",
    "# mm_data = sample_real_data(dataset.test_dataset[0], idx=50)\n",
    "mm_data = sample_real_data(dataset.train_dataset)\n",
    "print(mm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "image_batch = mm_data.images.read_images(size=mm_data.images.map_size_low)\n",
    "\n",
    "# Visualize\n",
    "visualize_mm_data(mm_data, class_names=CLASSES, class_colors=OBJECT_COLOR, image_batch=image_batch, figsize=1000, voxel=0.05, show_3d=True, show_2d=True, color_mode='pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.datasets.multimodal.data import MMData\n",
    "import copy\n",
    "\n",
    "# Change image resolution\n",
    "ratio = 2\n",
    "map_size_low_resized = tuple([int(x / ratio)for x in mm_data.images.map_size_low])\n",
    "image_batch_resized = mm_data.images.read_images(size=map_size_low_resized)\n",
    "\n",
    "# Change mapping 2D resolution\n",
    "mm_data_resized = copy.deepcopy(mm_data)  # use clone instead\n",
    "mm_data_resized.mappings = mm_data_resized.mappings.downscale_views(ratio)\n",
    "\n",
    "# Visualize\n",
    "visualize_mm_data(mm_data_resized, class_names=CLASSES, class_colors=OBJECT_COLOR, image_batch=image_batch_resized, figsize=1000, voxel=0.05, show_3d=True, show_2d=True, color_mode='pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize visibility models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_points3d.core.multimodal.visibility import visibility\n",
    "from torch_points3d.datasets.segmentation.multimodal.s3dis import s3dis_image_area\n",
    "from torch_points3d.core.multimodal.visibility import read_s3dis_depth_map\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "data = dataset.test_dataset[0]._datas[0]\n",
    "images = dataset.test_dataset[0]._images[0]\n",
    "\n",
    "img_selection = np.random.choice(np.arange(images.num_views), 5, replace=False)\n",
    "for i_image, im in zip(img_selection, images[img_selection]):\n",
    "    # img_size = (512, 256)\n",
    "    img_size = (256, 158)\n",
    "    # img_size = im.img_size\n",
    "\n",
    "    xyz_to_img = (data.pos - im.pos.squeeze()).float()\n",
    "    img_opk = im.opk.squeeze().float()\n",
    "    linearity = data.linearity\n",
    "    planarity = data.planarity\n",
    "    scattering = data.scattering\n",
    "    normals = data.norm\n",
    "    img_mask = torch.from_numpy(np.array(Image.fromarray(im.mask.t().numpy()).resize(img_size))).t()\n",
    "    \n",
    "    depth_threshold=0.05\n",
    "    depth_map_path = os.path.join(\n",
    "        DATA_ROOT, 'shared/image', s3dis_image_area(im.path[0]), 'pano/depth', \n",
    "        os.path.basename(im.path[0]).replace('_rgb.png', '_depth.png'))\n",
    "    use_cuda=True\n",
    "    \n",
    "    # Recover the real RGB and depth images\n",
    "    real_rgb = np.array(Image.open(os.path.join(DATA_ROOT, *im.path[0].split('/')[-7:])).resize((2*img_size[0], 2*img_size[1])))\n",
    "    real_depth_map = read_s3dis_depth_map(path, img_size=img_size, empty=-1)\n",
    "    real_depth_map[real_depth_map < im.r_min] = -1\n",
    "    real_depth_map[real_depth_map > im.r_max] = -1\n",
    "    real_depth_map[torch.from_numpy(np.logical_not(img_mask.numpy()))] = -1\n",
    "    \n",
    "    # Plot the RGB and depth images\n",
    "    fig, axes = plt.subplots(int((len(methods) + 2)/2), 2, figsize=(12*2, 8 * int((len(methods) + 2)/2)))\n",
    "    fig.suptitle(f'S3DIS Area 5 image {i_image}', fontsize=24)\n",
    "    axes[0, 0].imshow(real_rgb)\n",
    "    axes[0, 0].set_title('RGB')\n",
    "    depth_map_kwargs = {'cmap':'nipy_spectral', 'vmin':-im.r_max, 'vmax':-im.r_min, 'interpolation': 'nearest'}\n",
    "    axes[0, 1].imshow(-real_depth_map.t().numpy(), **depth_map_kwargs)\n",
    "    axes[0, 1].set_title('True Depth')\n",
    "    \n",
    "    methods = ['splatting', 'splatting_exact', 'depth_map', 'biasutti']\n",
    "    for i_method, method in enumerate(methods):\n",
    "                \n",
    "        out = visibility(\n",
    "            xyz_to_img, img_opk, linearity=linearity, planarity=planarity,\n",
    "            scattering=scattering, normals=normals, img_mask=img_mask,\n",
    "            img_size=img_size, voxel=im.voxel, r_max=im.r_max,\n",
    "            r_min=im.r_min, k_swell=im.growth_k, d_swell=im.growth_r,\n",
    "            exact='exact' in method, method=method.replace('_exact', ''), depth_threshold=depth_threshold, \n",
    "            depth_map_path=depth_map_path, use_cuda=use_cuda, biasutti_k=75,\n",
    "            biasutti_margin=64, biasutti_threshold=0.5)\n",
    "    \n",
    "        # proj_map = torch.zeros(img_size).long()\n",
    "        # proj_map[out['x'].long().cpu(), out['y'].long().cpu()] = out['idx'].cpu()\n",
    "\n",
    "        # proj_map = torch.zeros(img_size).float()\n",
    "        # proj_map[out['x'].long().cpu(), out['y'].long().cpu()] = out['features'][:, 0].cpu()\n",
    "\n",
    "        proj_map = torch.zeros(img_size).float()\n",
    "        proj_map[out['x'].long().cpu(), out['y'].long().cpu()] = out['depth'].cpu()\n",
    "        \n",
    "        # Plot the computed map\n",
    "        pcent_mapped_points = round(100 * out['idx'].shape[0] / xyz_to_img.shape[0], 1)\n",
    "        pcent_mapped_pixels = round(100 * n_mappings / (img_size[0] * img_size[1]), 1)\n",
    "        ax = axes[(i_method+2) // 2, (i_method+2) % 2]\n",
    "        ax.imshow(-proj_map.t().numpy(), **depth_map_kwargs)\n",
    "        ax.set_title(f'Visibility - {method} - {pcent_mapped_points}% points mapped - {pcent_mapped_pixels}% pixels mapped')\n",
    "   \n",
    "    plt.show()\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3DIS annotation errors\n",
    "\n",
    "### Room orientation errors\n",
    "- Area 1: None\n",
    "- Area 2: hallway_11 (+180°) \n",
    "- Area 3: None\n",
    "- Area 4: None\n",
    "- Area 5: hallway 6 (or 7?) (+180°)\n",
    "- Area 6: None\n",
    "\n",
    "### Annotation errors\n",
    "- Area 5: office_27 - ceiling points a bit everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tp3d_dev] *",
   "language": "python",
   "name": "conda-env-tp3d_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
